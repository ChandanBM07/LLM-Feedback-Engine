# -*- coding: utf-8 -*-
"""LLM Powered Feedback Engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HyeU8lODtW9CKsknKxOwya1mcBrUnPl6
"""

import pandas as pd

# Load datasets
google_df = pd.read_csv('Play Store Data.csv')
apple_df = pd.read_csv('Apple_Store_Reviews.csv')

# Inspect columns (you already printed this)
# Google: no real 'Review' column, only 'Reviews' (count)
# Apple: has 'Review_Text'

# Rename + standardize
google_df.rename(columns={
    'App': 'App_Name',
    'Rating': 'App_Rating',
    'Category': 'Category'
}, inplace=True)

apple_df.rename(columns={
    'App_Name': 'App_Name',
    'Rating': 'App_Rating',
    'Review_Text': 'Review',
    'Category': 'Category'
}, inplace=True)

# Add placeholder 'Review' column for Google
google_df['Review'] = 'No Review Available'

# Add source column
google_df['Source'] = 'Google Play Store'
apple_df['Source'] = 'Apple App Store'

# Select relevant columns
common_cols = ['App_Name', 'App_Rating', 'Review', 'Category', 'Source']
google_common = google_df[common_cols]
apple_common = apple_df[common_cols]

# Combine datasets
combined_df = pd.concat([google_common, apple_common], ignore_index=True)

# Save combined file
combined_df.to_csv('combined_app_reviews1.csv', index=False)
print("âœ… Combined dataset saved as 'combined_app_reviews1.csv'")

import pandas as pd
import numpy as np

# Load combined dataset
df = pd.read_csv('combined_app_reviews.csv')

print("Original shape:", df.shape)

# 1ï¸âƒ£ Remove duplicates
df = df.drop_duplicates()
print("After removing duplicates:", df.shape)

# 2ï¸âƒ£ Handle missing values
# For ratings â†’ fill with median (per app if possible)
if 'App_Rating' in df.columns:
    df['App_Rating'] = pd.to_numeric(df['App_Rating'], errors='coerce')  # ensure numeric
    df['App_Rating'] = df.groupby('App_Name')['App_Rating'].transform(lambda x: x.fillna(x.median()))
    # if still missing, fill with overall median
    df['App_Rating'] = df['App_Rating'].fillna(df['App_Rating'].median())

# For Review â†’ fill with "No Review"
df['Review'] = df['Review'].fillna('No Review').astype(str)

# For Category â†’ fill with 'Unknown'
df['Category'] = df['Category'].fillna('Unknown')

# 3ï¸âƒ£ Trim whitespace + lowercase text columns
text_cols = ['App_Name', 'Review', 'Category', 'Source']
for col in text_cols:
    df[col] = df[col].astype(str).str.strip().str.lower()

# 4ï¸âƒ£ Remove outliers (App_Rating > 5 or < 1)
df = df[(df['App_Rating'] >= 1) & (df['App_Rating'] <= 5)]
print("After removing rating outliers:", df.shape)

# 5ï¸âƒ£ Final check for any remaining missing values
print("Missing values summary:\n", df.isnull().sum())

# Save cleaned data
df.to_csv('super_cleaned_app_reviews.csv', index=False)
print("âœ… Super cleaned dataset saved as 'super_cleaned_app_reviews.csv'")

import pandas as pd
from textblob import TextBlob

# Load your super cleaned dataset
df = pd.read_csv('super_cleaned_app_reviews.csv')

# Function to compute sentiment polarity and label
def get_sentiment(text):
    blob = TextBlob(str(text))
    polarity = blob.sentiment.polarity
    if polarity > 0.1:
        label = 'Positive'
    elif polarity < -0.1:
        label = 'Negative'
    else:
        label = 'Neutral'
    return pd.Series([polarity, label])

# Apply sentiment analysis
df[['Sentiment_Polarity', 'Sentiment_Label']] = df['Review'].apply(get_sentiment)

# âœ… Group by Category and summarize
category_summary = df.groupby('Category')['Sentiment_Label'].value_counts().unstack().fillna(0)
print("\nðŸ“Š Sentiment Summary by Category:")
print(category_summary)

# âœ… Group by App_Name and summarize
app_summary = df.groupby('App_Name')['Sentiment_Label'].value_counts().unstack().fillna(0)
print("\nðŸ“Š Sentiment Summary by App:")
print(app_summary)

# âœ… Group by Ratings and summarize (if App_Rating exists)
if 'App_Rating' in df.columns:
    rating_summary = df.groupby('App_Rating')['Sentiment_Label'].value_counts().unstack().fillna(0)
    print("\nðŸ“Š Sentiment Summary by App Rating:")
    print(rating_summary)
else:
    print("\nâš ï¸ No 'App_Rating' column found.")

# Save enriched data
df.to_csv('sentiment_labeled_app_reviews.csv', index=False)
print("\nâœ… Saved: sentiment_labeled_app_reviews.csv")

import pandas as pd
from transformers import pipeline
from tqdm.auto import tqdm

# Load your cleaned dataset
df = pd.read_csv("super_cleaned_app_reviews.csv")

# âœ… Initialize Hugging Face pipelines
theme_classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# âœ… Define your custom theme labels
theme_labels = ["UI/UX", "Performance", "Bugs", "Features", "Customer Support", "Pricing", "Functionality"]

# âœ… Progress bar setup
tqdm.pandas()

# Step 1: Classify themes
def classify_theme(text):
    try:
        result = theme_classifier(text, candidate_labels=theme_labels)
        return result["labels"][0], result["scores"][0]
    except:
        return "Unknown", 0.0

df[["Theme", "Theme_Confidence"]] = df["Review"].progress_apply(lambda x: pd.Series(classify_theme(str(x))))

# Step 2: Summarize category-wise feedback
summaries = {}
for category in df["Category"].unique():
    subset = df[df["Category"] == category]["Review"].dropna().astype(str).tolist()
    text_chunk = " ".join(subset[:5])  # summarize first 5 reviews per category
    if len(text_chunk) > 50:
        try:
            summary = summarizer(text_chunk, max_length=80, min_length=25, do_sample=False)[0]["summary_text"]
            summaries[category] = summary
        except:
            summaries[category] = "Summary generation failed."
    else:
        summaries[category] = "Too little text to summarize."

# Map summaries back
df["Category_Summary"] = df["Category"].map(summaries)

# âœ… Save enhanced insights
output_path = "llm_enriched_feedback.csv"
df.to_csv(output_path, index=False)
print(f"âœ… Saved: {output_path}")

import requests
import pandas as pd
from tqdm import tqdm

# Replace this with your actual token
api_token = " "

# Load your CSV with reviews
df = pd.read_csv("llm_enriched_feedback.csv")
df = df.dropna(subset=["Review"])  # Ensure no nulls

# Setup Hugging Face API endpoint and headers
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
headers = {"Authorization": f"Bearer {api_token}"}

# Define labels for classification
labels = ["Bug Report", "Feature Request", "Praise", "Complaint"]

def classify(text):
    payload = {
        "inputs": text,
        "parameters": {"candidate_labels": labels}
    }
    response = requests.post(API_URL, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        return result["labels"][0], result["scores"][0]
    else:
        return "Unknown", 0.0

# Run classification and store results
tqdm.pandas()
df[["Category", "Confidence"]] = df["Review"].progress_apply(lambda x: pd.Series(classify(str(x))))

# Save to new file
df.to_csv("feedback_with_categories_api.csv", index=False)
print("âœ… Saved: feedback_with_categories_api.csv")

from huggingface_hub import login
login(token=" ")  # âœ… required for private or rate-limited models

from transformers import pipeline
import pandas as pd
from tqdm.auto import tqdm

# Load your file
df = pd.read_csv("llm_enriched_feedback.csv")

# Initialize sentiment model
sentiment_model = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

# Add sentiment
tqdm.pandas()

def get_sentiment(text):
    try:
        result = sentiment_model(text[:512])[0]  # limit input length
        return result["label"], result["score"]
    except:
        return "UNKNOWN", 0

df[["Sentiment_Label", "Sentiment_Score"]] = df["Review"].astype(str).progress_apply(
    lambda x: pd.Series(get_sentiment(x))
)

# Save updated file
df.to_csv("llm_enriched_feedback_with_sentiment.csv", index=False)
print("âœ… Added Sentiment_Label and saved as 'llm_enriched_feedback_with_sentiment.csv'")

import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("llm_enriched_feedback_with_sentiment.csv")

plt.figure(figsize=(8, 5))
sns.countplot(data=df, x="Sentiment_Label", palette="Set2")
plt.title("LLM-Powered Sentiment Distribution")
plt.tight_layout()
plt.show()

import pandas as pd
from transformers import pipeline

# Load data
df = pd.read_csv("llm_enriched_feedback.csv")

# Clean placeholder reviews
df["Review"] = df["Review"].fillna("").astype(str)
df["Review_Lower"] = df["Review"].str.lower()

# Define reviews with no usable content
placeholder_phrases = [
    "no review available", "no reviews available", "no comment", "n/a", "none", ""
]
df["Is_Empty_Review"] = df["Review_Lower"].apply(lambda x: any(p in x for p in placeholder_phrases))

# Load Hugging Face sentiment model
sentiment_model = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

# Define function: use text if available, else use rating
def hybrid_sentiment(row):
    if not row["Is_Empty_Review"]:
        try:
            result = sentiment_model(row["Review"][:512])[0]  # Truncate long reviews
            return result["label"], float(result["score"])
        except:
            return "NEUTRAL", 0.5
    else:
        rating = row.get("App_Rating", 0)
        if rating >= 4:
            return "POSITIVE", 1.0
        elif rating <= 2:
            return "NEGATIVE", 1.0
        else:
            return "NEUTRAL", 1.0

# Apply hybrid sentiment logic
print("ðŸ” Applying hybrid sentiment classification...")
df[["Hybrid_Sentiment_Label", "Hybrid_Sentiment_Confidence"]] = df.apply(hybrid_sentiment, axis=1, result_type="expand")

# Save updated dataset
df.drop(columns=["Review_Lower", "Is_Empty_Review"], inplace=True)
df.to_csv("hybrid_sentiment_output.csv", index=False)
print("âœ… Saved: hybrid_sentiment_output.csv")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv("hybrid_sentiment_output.csv")

# Check available sentiment labels
print(df["Sentiment_Label"].value_counts())

# Plot Sentiment Distribution
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x="Sentiment_Label", palette="Set2")
plt.title("Hybrid LLM Sentiment Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Review Count")
plt.tight_layout()
plt.show()

# Sentiment per App
top_apps = df['App_Name'].value_counts().head(5).index.tolist()
subset = df[df['App_Name'].isin(top_apps)]

plt.figure(figsize=(12, 6))
sns.countplot(data=subset, x="App_Name", hue="Sentiment_Label", palette="coolwarm")
plt.title("Sentiment Breakdown for Top 5 Apps")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
from transformers import pipeline

# Load data (make sure your source CSV has 'review date' column)
df = pd.read_csv("llm_enriched_feedback.csv")

# Clean placeholder reviews
df["Review"] = df["Review"].fillna("").astype(str)
df["Review_Lower"] = df["Review"].str.lower()

placeholder_phrases = [
    "no review available", "no reviews available", "no comment", "n/a", "none", ""
]
df["Is_Empty_Review"] = df["Review_Lower"].apply(lambda x: any(p in x for p in placeholder_phrases))

# Load Hugging Face sentiment model
sentiment_model = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

def hybrid_sentiment(row):
    if not row["Is_Empty_Review"]:
        try:
            result = sentiment_model(row["Review"][:512])[0]
            return result["label"], float(result["score"])
        except:
            return "NEUTRAL", 0.5
    else:
        rating = row.get("App_Rating", 0)
        if rating >= 4:
            return "POSITIVE", 1.0
        elif rating <= 2:
            return "NEGATIVE", 1.0
        else:
            return "NEUTRAL", 1.0

print("ðŸ” Applying hybrid sentiment classification...")
df[["Hybrid_Sentiment_Label", "Hybrid_Sentiment_Confidence"]] = df.apply(hybrid_sentiment, axis=1, result_type="expand")

# Clean up helper columns
df.drop(columns=["Review_Lower", "Is_Empty_Review"], inplace=True)

# Make sure 'review date' column is still there before saving
print(df.columns)  # Just to confirm 'review date' exists

# Save to CSV including review date
df.to_csv("hybrid_sentiment_output_with_date.csv", index=False)
print("âœ… Saved: hybrid_sentiment_output_with_date.csv with review date included")

import pandas as pd
from transformers import pipeline

# Load enriched feedback (no Review_ID)
df = pd.read_csv("llm_enriched_feedback.csv")

# Load Play Store source data (has App_Name + review date)
playstore_df = pd.read_csv("Play Store Data.csv")

# Merge to bring in 'review date' based on App_Name
df = df.merge(
    playstore_df[['App_Name', 'Review_Date']],
    on='App_Name',
    how='left'
)

# Optional: check how many rows got matched
matched = df['Review_Date'].notna().sum()
missing = df['Review_Date'].isna().sum()
print(f"âœ… Review date matched for {matched} rows; missing in {missing} rows.")


from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import pandas as pd
# Load enriched feedback (no Review_ID)
df = pd.read_csv("hybrid_sentiment_output.csv")
# Ensure the correct sentiment column is used
if 'Sentiment_Label' in df.columns:
    label_col = 'Sentiment_Label'
elif 'Sentiment_Lable' in df.columns:
    label_col = 'Sentiment_Lable'
else:
    raise ValueError("âŒ Neither 'Sentiment_Label' nor 'Sentiment_Lable' found in dataframe columns.")

# Focus on actionable feedback (NEGATIVE or NEUTRAL)
feedback_df = df[df[label_col].isin(['NEGATIVE', 'NEUTRAL'])]

# Extract common bi-grams and tri-grams
vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words='english', max_features=50)
X = vectorizer.fit_transform(feedback_df['Review'].astype(str))  # Ensure it's string
features = vectorizer.get_feature_names_out()
counts = X.sum(axis=0).A1
feature_freq = pd.DataFrame({'Feature': features, 'Frequency': counts}).sort_values(by='Frequency', ascending=False)

# Horizontal Bar Chart
plt.figure(figsize=(12, 5))
plt.barh(feature_freq['Feature'][:15][::-1], feature_freq['Frequency'][:15][::-1], color='salmon')
plt.title("Top Feature Requests in NEGATIVE / NEUTRAL Reviews")
plt.xlabel("Frequency")
plt.tight_layout()
plt.show()

# Word Cloud
wc = WordCloud(width=1000, height=400, background_color='white').generate_from_frequencies(dict(zip(features, counts)))
plt.figure(figsize=(12, 5))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud of Feature Phrases")
plt.show()

import os
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from transformers import pipeline
from huggingface_hub import login

# Step 1: Login with your Hugging Face token (replace with your token)
login(" ")  # <-- Replace this string with your real token

# Step 2: Load your dataset
df = pd.read_csv("hybrid_sentiment_output.csv")

# Step 3: Setup zero-shot pipeline, huggingface will use your logged-in token automatically
# Check if GPU available, else CPU
import torch
device = 0 if torch.cuda.is_available() else -1
print(f"Using device: {'GPU' if device == 0 else 'CPU'}")

zero_shot = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli",
    device=device,
    use_auth_token=True  # explicitly use token to authenticate
)

topics = ["user interface", "performance", "bugs", "features", "pricing", "ads", "login issues"]

BATCH_SIZE = 16

def classify_batch(texts):
    try:
        results = zero_shot(texts, topics)
        return [res['labels'][0] for res in results]
    except Exception as e:
        print("Batch classification error:", e)
        return ["unclassified"] * len(texts)

reviews = df['Review'].fillna("").astype(str).str[:512].tolist()

all_topics = []
print("ðŸ” Classifying review topics in batches...")

for i in tqdm(range(0, len(reviews), BATCH_SIZE)):
    batch = reviews[i:i+BATCH_SIZE]
    batch_topics = classify_batch(batch)
    all_topics.extend(batch_topics)

df['Topic'] = all_topics

# Plot the results
topic_counts = df['Topic'].value_counts().sort_values(ascending=True)
plt.figure(figsize=(10, 6))
topic_counts.plot(kind='barh', color='skyblue')
plt.title("Most Discussed Topics in Reviews")
plt.xlabel("Number of Mentions")
plt.tight_layout()
plt.show()

df.to_csv("llm_topic_classified_reviews.csv", index=False)
print("âœ… Saved: llm_topic_classified_reviews.csv")

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.countplot(data=df, y="Topic", hue="Sentiment_Label", order=df['Topic'].value_counts().index, palette="coolwarm")
plt.title("Sentiment Distribution by Topic")
plt.xlabel("Review Count")
plt.ylabel("Topic")
plt.tight_layout()
plt.show()

top_negative = df[df["Sentiment_Label"] == "NEGATIVE"]["Topic"].value_counts().head(5)
print("Top 5 Problem Areas (Negative Sentiment):")
print(top_negative)

summary = {
    "Total Reviews": len(df),
    "Positive Reviews": (df["Sentiment_Label"] == "POSITIVE").sum(),
    "Negative Reviews": (df["Sentiment_Label"] == "NEGATIVE").sum(),
    "Neutral Reviews": (df["Sentiment_Label"] == "NEUTRAL").sum(),
    "Most Common Topic": df["Topic"].value_counts().idxmax()
}

for k, v in summary.items():
    print(f"{k}: {v}")

from transformers import pipeline
import pandas as pd

# Load the dataset (if not already loaded)
df = pd.read_csv("llm_topic_classified_reviews.csv")

# Filter negative reviews
negative_df = df[df["Sentiment_Label"] == "NEGATIVE"]

# Load summarization model
summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=0 if torch.cuda.is_available() else -1)

# Define function to summarize text per topic
def summarize_reviews(topic, max_reviews=20):
    topic_reviews = negative_df[negative_df["Topic"] == topic]["Review"].dropna().astype(str).tolist()
    selected_reviews = topic_reviews[:max_reviews]  # avoid input length overflow
    combined_text = " ".join(selected_reviews)

    if len(combined_text.strip()) < 30:
        return "Not enough data to summarize."

    try:
        summary = summarizer(combined_text[:1024], max_length=100, min_length=30, do_sample=False)
        return summary[0]['summary_text']
    except Exception as e:
        print(f"Error summarizing topic '{topic}':", e)
        return "Summarization failed."

# Summarize each topic
topics = negative_df["Topic"].unique()
summaries = {}

print("ðŸ“„ Generating summaries for negative reviews per topic...")

for topic in topics:
    summaries[topic] = summarize_reviews(topic)

# Print the summaries
for topic, summary in summaries.items():
    print(f"\nðŸ”¹ Topic: {topic}\nSummary: {summary}\n")

with open("negative_review_summaries.txt", "w") as f:
    for topic, summary in summaries.items():
        f.write(f"Topic: {topic}\nSummary: {summary}\n\n")
print("ðŸ“ Saved: negative_review_summaries.txt")

import pandas as pd

# --- Load your processed dataset ---
df = pd.read_csv("hybrid_sentiment_output.csv")

# --- Clean & Normalize App_Name ---
df["App_Name"] = df["App_Name"].astype(str).str.strip().str.lower()

# Optional: Remove clearly invalid app names (symbols, blanks, etc.)
df = df[df["App_Name"].str.match(r"^[a-zA-Z0-9\s\.\-_:]+$")]

# --- Step 1: Sentiment distribution per app ---
sentiment_summary = (
    df.groupby("App_Name")["Sentiment_Label"]
    .value_counts(normalize=True)
    .unstack(fill_value=0)
    .reset_index()
)

# Rename columns for clarity
sentiment_summary.columns.name = None
sentiment_summary.rename(columns={
    "POSITIVE": "Positive(%)",
    "NEGATIVE": "Negative(%)",
    "NEUTRAL": "Neutral(%)"
}, inplace=True)

# --- Step 2: Total number of reviews per app ---
review_counts = df["App_Name"].value_counts().reset_index()
review_counts.columns = ["App_Name", "Total_Reviews"]

# --- Step 3: Most common topic per app (if 'Topic' exists) ---
if "Topic" in df.columns:
    top_topics = (
        df.groupby("App_Name")["Topic"]
        .agg(lambda x: x.value_counts().idxmax())
        .reset_index()
        .rename(columns={"Topic": "Top_Topic"})
    )
else:
    top_topics = pd.DataFrame(columns=["App_Name", "Top_Topic"])
    print("âš ï¸ 'Topic' column not found. Skipping topic analysis.")

# --- Step 4: Merge summaries ---
summary_df = sentiment_summary.merge(review_counts, on="App_Name", how="left")
if not top_topics.empty:
    summary_df = summary_df.merge(top_topics, on="App_Name", how="left")

# --- Step 5: Round % columns and sort by Total Reviews ---
for col in ["Positive(%)", "Negative(%)", "Neutral(%)"]:
    if col in summary_df.columns:
        summary_df[col] = (summary_df[col] * 100).round(1)

summary_df = summary_df.sort_values(by="Total_Reviews", ascending=False)

# --- Step 6: Save output ---
summary_df.to_csv("app_level_summary_report.csv", index=False)
print("âœ… Saved: app_level_summary_report.csv")

# --- Optional Preview ---
print("\nðŸ“Š Top 5 Apps by Review Volume:")
print(summary_df.head())

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import numpy as np

# Load data
df = pd.read_csv("hybrid_sentiment_output.csv")

# Handle missing 'Date' column
if 'Date' not in df.columns:
    st.warning("âš ï¸ 'Date' column not found. Generating random dates for demonstration.")
    df['Date'] = pd.to_datetime(np.random.choice(pd.date_range("2024-01-01", "2024-12-31"), size=len(df)))
else:
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

# Sidebar filters
st.sidebar.header("ðŸ“Š Filter Options")
app_list = sorted(df["App_Name"].dropna().unique())
sentiments = sorted(df["Sentiment_Label"].dropna().unique())
topics = sorted(df["Topic"].dropna().unique()) if "Topic" in df.columns else []

# Filters
selected_apps = st.sidebar.multiselect("Select App(s)", app_list, default=app_list[:5])
selected_sentiments = st.sidebar.multiselect("Select Sentiment(s)", sentiments, default=sentiments)
if topics:
    selected_topics = st.sidebar.multiselect("Select Topic(s)", topics, default=topics)
search_term = st.sidebar.text_input("ðŸ” Search in Reviews", "")

# Filter dataset
filtered_df = df[df["App_Name"].isin(selected_apps) & df["Sentiment_Label"].isin(selected_sentiments)]
if topics:
    filtered_df = filtered_df[filtered_df["Topic"].isin(selected_topics)]
if search_term:
    filtered_df = filtered_df[filtered_df["Review"].str.contains(search_term, case=False, na=False)]

# Title
st.title("ðŸ“± LLM Product Feedback Dashboard")

# View toggle
view_option = st.radio("Select Visualization", ["Sentiment Breakdown", "Topic Distribution", "Word Cloud", "Sentiment Trend Over Time"])

# 1. Sentiment Breakdown
if view_option == "Sentiment Breakdown":
    st.subheader("ðŸ“Š Sentiment Breakdown by App")
    top_apps = filtered_df["App_Name"].value_counts().head(5).index.tolist()
    subset = filtered_df[filtered_df["App_Name"].isin(top_apps)]

    plt.figure(figsize=(10, 5))
    sns.countplot(data=subset, x="App_Name", hue="Sentiment_Label", palette="Set2")
    plt.title("Sentiment Breakdown (Top 5 Apps)")
    plt.xticks(rotation=45)
    st.pyplot(plt.gcf())

# 2. Topic Distribution
elif view_option == "Topic Distribution":
    st.subheader("ðŸ§  Topic Distribution")
    if "Topic" in filtered_df.columns:
        topic_counts = filtered_df["Topic"].value_counts()
        st.bar_chart(topic_counts)
    else:
        st.warning("âš ï¸ 'Topic' column not found in dataset.")

# 3. Word Cloud
elif view_option == "Word Cloud":
    st.subheader("â˜ï¸ Review Word Cloud")
    all_text = " ".join(filtered_df["Review"].dropna().astype(str))
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    st.pyplot(plt.gcf())

# 4. Sentiment Trend Over Time
elif view_option == "Sentiment Trend Over Time":
    st.subheader("ðŸ“ˆ Sentiment Trend Over Time")
    if 'Date' in filtered_df.columns:
        trend = (
            filtered_df.groupby([pd.Grouper(key='Date', freq='W'), 'Hybrid_Sentiment_Label'])
            .size()
            .unstack(fill_value=0)
            .sort_index()
        )
        st.line_chart(trend)
        st.caption("Track how user sentiment evolves weekly. Useful for monitoring app updates or campaigns.")
    else:
        st.warning("âš ï¸ No 'Date' column available for time series analysis.")

# Show filtered data
with st.expander("ðŸ“„ View Filtered Data"):
    st.dataframe(filtered_df.reset_index(drop=True))


import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns

# Load data
df = pd.read_csv("hybrid_sentiment_output.csv")

# Sidebar filters
st.sidebar.header("ðŸ“Š Filter Options")

# Unique values for filters
app_list = sorted(df["App_Name"].dropna().unique())
sentiments = sorted(df["Sentiment_Label"].dropna().unique())
topics = sorted(df["Topic"].dropna().unique()) if "Topic" in df.columns else []

# App filter
selected_apps = st.sidebar.multiselect("Select App(s)", app_list, default=app_list[:5])
# Sentiment filter
selected_sentiments = st.sidebar.multiselect("Select Sentiment(s)", sentiments, default=sentiments)
# Topic filter (if exists)
if topics:
    selected_topics = st.sidebar.multiselect("Select Topic(s)", topics, default=topics)

# Keyword search
search_term = st.sidebar.text_input("ðŸ” Search in Reviews", "")

# Filter data
filtered_df = df[df["App_Name"].isin(selected_apps) & df["Sentiment_Label"].isin(selected_sentiments)]
if topics:
    filtered_df = filtered_df[filtered_df["Topic"].isin(selected_topics)]
if search_term:
    filtered_df = filtered_df[filtered_df["Review"].str.contains(search_term, case=False, na=False)]

st.title("ðŸ“± LLM Product Feedback Dashboard")

# Toggle view
view_option = st.radio("Select Visualization", ["Sentiment Breakdown", "Topic Distribution", "Word Cloud"])

# 1. Sentiment Breakdown
if view_option == "Sentiment Breakdown":
    st.subheader("ðŸ“Š Sentiment Breakdown by App")
    top_apps = filtered_df["App_Name"].value_counts().head(5).index.tolist()
    subset = filtered_df[filtered_df["App_Name"].isin(top_apps)]

    plt.figure(figsize=(10, 5))
    sns.countplot(data=subset, x="App_Name", hue="Sentiment_Label", palette="coolwarm")
    plt.title("Sentiment Breakdown (Top 5 Apps)")
    plt.xticks(rotation=45)
    st.pyplot(plt.gcf())

# 2. Topic Distribution
elif view_option == "Topic Distribution":
    st.subheader("ðŸ§  Topic Distribution")
    if "Topic" in filtered_df.columns:
        topic_counts = filtered_df["Topic"].value_counts()
        st.bar_chart(topic_counts)
    else:
        st.warning("âš ï¸ 'Topic' column not found in dataset.")

# 3. Word Cloud
elif view_option == "Word Cloud":
    st.subheader("â˜ï¸ Review Word Cloud")
    all_text = " ".join(filtered_df["Review"].dropna().astype(str))
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    st.pyplot(plt.gcf())

# Show raw data
with st.expander("ðŸ—‚ View Filtered Data"):
    st.dataframe(filtered_df.reset_index(drop=True))

# 5. Model Evaluation â€“ Confidence Distribution
if "Sentiment_Confidence" in filtered_df.columns:
    st.subheader("ðŸŽ¯ Sentiment Confidence Distribution")

    # Basic stats
    st.write("**Confidence Summary Stats**")
    st.write(filtered_df["Sentiment_Confidence"].describe())

    # Plot histogram
    plt.figure(figsize=(10, 5))
    sns.histplot(filtered_df["Sentiment_Confidence"], bins=20, kde=True, color='green')
    plt.title("Distribution of Sentiment Confidence Scores")
    plt.xlabel("Confidence Score")
    plt.ylabel("Number of Predictions")
    st.pyplot(plt.gcf())

    # Box plot per sentiment (if applicable)
    if "Hybrid_Sentiment_Label" in filtered_df.columns:
        st.write("**Confidence per Sentiment**")
        plt.figure(figsize=(8, 4))
        sns.boxplot(data=filtered_df, x="Hybrid_Sentiment_Label", y="Sentiment_Confidence", palette="Set2")
        plt.title("Confidence Scores by Sentiment")
        st.pyplot(plt.gcf())

else:
    st.warning("âš ï¸ 'Sentiment_Confidence' column not found. Add model confidence scores to enable evaluation.")

!pip install -q streamlit transformers sentencepiece wordcloud seaborn pyngrok

# Authenticate with Hugging Face (optional for private models)
from huggingface_hub import login
login(" ")  # Optional: only if needed

import pandas as pd
import re

# Load the uploaded CSV
df = pd.read_csv("hybrid_sentiment_output.csv")

# Define patterns commonly used in feature requests
feature_keywords = [
    r"\bi wish\b",
    r"\bi would love\b",
    r"\bplease add\b",
    r"\bwould be great\b",
    r"\bit would be nice\b",
    r"\bplease include\b",
    r"\bhope you can\b",
    r"\bcan you add\b",
    r"\bit needs\b",
    r"\bmissing feature\b"
]

# Compile the regex
pattern = re.compile("|".join(feature_keywords), flags=re.IGNORECASE)

# Add a column 'Feature_Request' based on pattern match in 'Review'
df["Feature_Request"] = df["Review"].fillna("").apply(lambda x: bool(pattern.search(x)))

# Filter only feature request reviews
feature_requests_df = df[df["Feature_Request"] == True]

# Display result
print("ðŸ” Found", len(feature_requests_df), "feature request reviews.\n")
print(feature_requests_df[["App_Name", "Review", "Sentiment_Label"]].head(10))

# Optional: Save to new CSV
# feature_requests_df.to_csv("/mnt/data/feature_requests_output.csv", index=False)

import pandas as pd
import re

# Load data
df = pd.read_csv("hybrid_sentiment_output.csv")

# Broader phrases (no strict "I wish", etc.)
feature_keywords = [
    r"wish",
    r"would love",
    r"please add",
    r"should have",
    r"missing",
    r"need.*feature",
    r"need.*option",
    r"can you add",
    r"must have",
    r"important feature",
    r"could use",
    r"would be helpful",
    r"add.*support",
    r"add.*option",
    r"include.*feature",
    r"hope.*add",
    r"should include",
    r"more.*features"
]

pattern = re.compile("|".join(feature_keywords), flags=re.IGNORECASE)

# Apply pattern search
df["Feature_Request"] = df["Review"].fillna("").apply(lambda x: bool(pattern.search(x)))

# Filter
feature_requests_df = df[df["Feature_Request"] == True]

# Show results
print(f"ðŸ” Found {len(feature_requests_df)} feature request reviews.\n")
print(feature_requests_df[["App_Name", "Review", "Sentiment_Label"]].head(10))