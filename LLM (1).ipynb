{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74HFChLiswMj"
      },
      "outputs": [],
      "source": [
        "# app_feedback_pipeline.py\n",
        "# ------------------------------------------------------------\n",
        "# End-to-end pipeline for app-store review analytics\n",
        "# - Ingest + clean (Google Play + Apple)\n",
        "# - ML sentiment (DistilBERT SST-2) with hybrid fallback\n",
        "# - Zero-shot topic tagging (BART-MNLI), batched\n",
        "# - Optional summaries per topic (BART-CNN)\n",
        "# - N-gram mining & feature-request flags\n",
        "# - App-level summary report\n",
        "#\n",
        "# Configuration is at the top; all outputs go to ./outputs/.\n",
        "# Set HUGGINGFACE_TOKEN in your environment for Inference API use (optional).\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Transformers (local pipelines)\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Sklearn for n-grams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# ----------------------\n",
        "# CONFIG\n",
        "# ----------------------\n",
        "DATA_DIR = Path(\".\")\n",
        "OUT_DIR = Path(\"./outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Input file names (adjust if yours differ)\n",
        "PLAY_FILE = DATA_DIR / \"Play Store Data.csv\"\n",
        "APPLE_FILE = DATA_DIR / \"Apple_Store_Reviews.csv\"\n",
        "\n",
        "# Canonical outputs\n",
        "COMBINED_CSV = OUT_DIR / \"combined_app_reviews.csv\"\n",
        "CLEAN_CSV = OUT_DIR / \"super_cleaned_app_reviews.csv\"\n",
        "ML_SENT_CSV = OUT_DIR / \"ml_sentiment_reviews.csv\"\n",
        "HYBRID_SENT_CSV = OUT_DIR / \"hybrid_sentiment_reviews.csv\"\n",
        "TOPIC_CSV = OUT_DIR / \"topic_classified_reviews.csv\"\n",
        "APP_SUMMARY_CSV = OUT_DIR / \"app_level_summary_report.csv\"\n",
        "NEG_SUMMARY_TXT = OUT_DIR / \"negative_review_summaries.txt\"\n",
        "NGRAMS_CSV = OUT_DIR / \"top_ngrams_negative_neutral.csv\"\n",
        "FEATURE_REQUESTS_CSV = OUT_DIR / \"feature_requests.csv\"\n",
        "\n",
        "# Models\n",
        "MODEL_SENTIMENT = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "MODEL_ZS = \"facebook/bart-large-mnli\"\n",
        "MODEL_SUM = \"facebook/bart-large-cnn\"\n",
        "\n",
        "# Zero-shot labels (use multi_label to allow multiple topics)\n",
        "ZS_LABELS = [\"user interface\", \"performance\", \"bugs\", \"features\", \"pricing\", \"ads\", \"login issues\", \"customer support\"]\n",
        "ZS_MULTI_LABEL = True\n",
        "ZS_SCORE_THRESH = 0.35  # keep labels scoring >= threshold when multi_label=True\n",
        "\n",
        "# Placeholder review detection\n",
        "PLACEHOLDERS = [\n",
        "    \"no review available\", \"no reviews available\", \"no comment\",\n",
        "    \"n/a\", \"none\", \"\", \"null\"\n",
        "]\n",
        "\n",
        "# Hybrid sentiment thresholds (when no usable text)\n",
        "RATING_POS = 4.0\n",
        "RATING_NEG = 2.0\n",
        "\n",
        "# Chunking for summarization\n",
        "MAX_REVIEWS_PER_TOPIC_FOR_SUMMARY = 50  # safety on input length\n",
        "SUMMARY_MAX_LEN = 120\n",
        "SUMMARY_MIN_LEN = 40\n",
        "\n",
        "# ----------------------\n",
        "# UTILS\n",
        "# ----------------------\n",
        "def safe_lower(s: str) -> str:\n",
        "    try:\n",
        "        return str(s).strip().lower()\n",
        "    except Exception:\n",
        "        return str(s)\n",
        "\n",
        "def is_placeholder_text(s: str) -> bool:\n",
        "    s = safe_lower(s)\n",
        "    return any(p == s or p in s for p in PLACEHOLDERS)\n",
        "\n",
        "def batch_iter(lst: List[str], batch_size: int):\n",
        "    for i in range(0, len(lst), batch_size):\n",
        "        yield lst[i:i + batch_size], i\n",
        "\n",
        "def ensure_columns(df: pd.DataFrame, cols: List[str]):\n",
        "    for c in cols:\n",
        "        if c not in df.columns:\n",
        "            df[c] = np.nan\n",
        "    return df\n",
        "\n",
        "def map_sst2_label(label: str) -> str:\n",
        "    # Normalize to UPPER fixed set\n",
        "    label = safe_lower(label)\n",
        "    if \"pos\" in label:\n",
        "        return \"POSITIVE\"\n",
        "    if \"neg\" in label:\n",
        "        return \"NEGATIVE\"\n",
        "    return \"NEUTRAL\"\n",
        "\n",
        "# ----------------------\n",
        "# 1) INGEST + STANDARDIZE\n",
        "# ----------------------\n",
        "def ingest_and_standardize() -> pd.DataFrame:\n",
        "    # Load\n",
        "    g = pd.read_csv(PLAY_FILE)\n",
        "    a = pd.read_csv(APPLE_FILE)\n",
        "\n",
        "    # Standardize names\n",
        "    g = g.rename(columns={\n",
        "        \"App\": \"App_Name\",\n",
        "        \"Rating\": \"App_Rating\",\n",
        "        \"Category\": \"Category\",\n",
        "        # if your Play CSV has review text, map here, else we'll inject placeholder\n",
        "    })\n",
        "    a = a.rename(columns={\n",
        "        \"App_Name\": \"App_Name\",\n",
        "        \"Rating\": \"App_Rating\",\n",
        "        \"Review_Text\": \"Review\",\n",
        "        \"Category\": \"Category\"\n",
        "    })\n",
        "\n",
        "    # Add review text for Google if missing\n",
        "    if \"Review\" not in g.columns:\n",
        "        g[\"Review\"] = \"No Review Available\"\n",
        "\n",
        "    # Source\n",
        "    g[\"Source\"] = \"Google Play Store\"\n",
        "    a[\"Source\"] = \"Apple App Store\"\n",
        "\n",
        "    # Keep canonical columns if present\n",
        "    common = [\"App_Name\", \"App_Rating\", \"Review\", \"Category\", \"Source\"]\n",
        "    g = ensure_columns(g, common)[common]\n",
        "    a = ensure_columns(a, common)[common]\n",
        "\n",
        "    df = pd.concat([g, a], ignore_index=True)\n",
        "    df.to_csv(COMBINED_CSV, index=False)\n",
        "    return df\n",
        "\n",
        "# ----------------------\n",
        "# 2) CLEANING\n",
        "# ----------------------\n",
        "def clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.drop_duplicates().copy()\n",
        "\n",
        "    # Coerce rating\n",
        "    if \"App_Rating\" in df.columns:\n",
        "        df[\"App_Rating\"] = pd.to_numeric(df[\"App_Rating\"], errors=\"coerce\")\n",
        "        # per-app median then global\n",
        "        df[\"App_Rating\"] = df.groupby(\"App_Name\")[\"App_Rating\"].transform(lambda x: x.fillna(x.median()))\n",
        "        df[\"App_Rating\"] = df[\"App_Rating\"].fillna(df[\"App_Rating\"].median())\n",
        "\n",
        "    # Fill missing text fields\n",
        "    df[\"Review\"] = df[\"Review\"].fillna(\"No Review\")\n",
        "    df[\"Category\"] = df[\"Category\"].fillna(\"Unknown\")\n",
        "    df[\"Source\"] = df[\"Source\"].fillna(\"unknown\")\n",
        "    df[\"App_Name\"] = df[\"App_Name\"].fillna(\"unknown_app\")\n",
        "\n",
        "    # Normalize (lowercase/strip for processing); keep a display copy if needed\n",
        "    for col in [\"App_Name\", \"Review\", \"Category\", \"Source\"]:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "    # Bound ratings to [1, 5]\n",
        "    if \"App_Rating\" in df.columns:\n",
        "        df = df[(df[\"App_Rating\"] >= 1.0) & (df[\"App_Rating\"] <= 5.0)]\n",
        "\n",
        "    # Add placeholder flag\n",
        "    df[\"Is_Empty_Review\"] = df[\"Review\"].apply(is_placeholder_text)\n",
        "\n",
        "    df.to_csv(CLEAN_CSV, index=False)\n",
        "    return df\n",
        "\n",
        "# ----------------------\n",
        "# 3) SENTIMENT (ML, BATCHED)\n",
        "# ----------------------\n",
        "def run_ml_sentiment(df: pd.DataFrame, text_col=\"Review\", max_len=512, batch_size=32) -> pd.DataFrame:\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    clf = pipeline(\"sentiment-analysis\", model=MODEL_SENTIMENT, device=device)\n",
        "\n",
        "    texts = df[text_col].astype(str).tolist()\n",
        "    results = []\n",
        "    for batch, i0 in tqdm(batch_iter(texts, batch_size), total=math.ceil(len(texts)/batch_size), desc=\"ML Sentiment\"):\n",
        "        # truncate to max_len (rough; tokenization occurs inside pipeline)\n",
        "        batch = [t[:max_len] for t in batch]\n",
        "        try:\n",
        "            out = clf(batch)\n",
        "        except Exception as e:\n",
        "            # fallback: neutral for the whole batch on error\n",
        "            out = [{\"label\": \"NEUTRAL\", \"score\": 0.0} for _ in batch]\n",
        "        for r in out:\n",
        "            results.append((map_sst2_label(r.get(\"label\", \"\")), float(r.get(\"score\", 0.0))))\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"Sentiment_Label\"] = [r[0] for r in results]\n",
        "    df[\"Sentiment_Confidence\"] = [r[1] for r in results]\n",
        "    df.to_csv(ML_SENT_CSV, index=False)\n",
        "    return df\n",
        "\n",
        "# ----------------------\n",
        "# 4) HYBRID SENTIMENT (text if available, else rating)\n",
        "# ----------------------\n",
        "def apply_hybrid_sentiment(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "\n",
        "    # If review is placeholder/empty ‚Üí fall back to rating\n",
        "    def hybrid(row):\n",
        "        if not row.get(\"Is_Empty_Review\", False):\n",
        "            # use ML result\n",
        "            return row.get(\"Sentiment_Label\", \"NEUTRAL\"), float(row.get(\"Sentiment_Confidence\", 0.0))\n",
        "        # rating fallback\n",
        "        rating = float(row.get(\"App_Rating\", 0.0)) if not pd.isna(row.get(\"App_Rating\", np.nan)) else 0.0\n",
        "        if rating >= RATING_POS:\n",
        "            return \"POSITIVE\", 1.0\n",
        "        elif rating <= RATING_NEG:\n",
        "            return \"NEGATIVE\", 1.0\n",
        "        else:\n",
        "            return \"NEUTRAL\", 1.0\n",
        "\n",
        "    hybrid_labels, hybrid_conf = [], []\n",
        "    for _, r in df.iterrows():\n",
        "        lab, conf = hybrid(r)\n",
        "        hybrid_labels.append(lab)\n",
        "        hybrid_conf.append(conf)\n",
        "\n",
        "    df[\"Hybrid_Sentiment_Label\"] = hybrid_labels\n",
        "    df[\"Hybrid_Sentiment_Confidence\"] = hybrid_conf\n",
        "    df.to_csv(HYBRID_SENT_CSV, index=False)\n",
        "    return df\n",
        "\n",
        "# ----------------------\n",
        "# 5) ZERO-SHOT TOPIC TAGGING (BATCHED)\n",
        "# ----------------------\n",
        "def zero_shot_topics(df: pd.DataFrame, text_col=\"Review\", batch_size=16) -> pd.DataFrame:\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    zsp = pipeline(\"zero-shot-classification\", model=MODEL_ZS, device=device)\n",
        "\n",
        "    # Only run on non-placeholder text to save time\n",
        "    mask = ~df[\"Is_Empty_Review\"].fillna(False)\n",
        "    texts = df.loc[mask, text_col].astype(str).str.slice(0, 512).tolist()\n",
        "    idxs = df.index[mask].tolist()\n",
        "\n",
        "    topic_col = pd.Series(index=df.index, dtype=object)\n",
        "    topics_multi = []\n",
        "\n",
        "    for batch, i0 in tqdm(batch_iter(texts, batch_size), total=math.ceil(len(texts)/batch_size), desc=\"Zero-Shot Topics\"):\n",
        "        try:\n",
        "            out = zsp(batch, candidate_labels=ZS_LABELS, multi_label=ZS_MULTI_LABEL)\n",
        "        except Exception as e:\n",
        "            # mark all as unknown on error\n",
        "            if ZS_MULTI_LABEL:\n",
        "                out = [{\"labels\": [], \"scores\": []} for _ in batch]\n",
        "            else:\n",
        "                out = [{\"labels\": [\"unclassified\"], \"scores\": [0.0]} for _ in batch]\n",
        "\n",
        "        for j, res in enumerate(out):\n",
        "            if ZS_MULTI_LABEL:\n",
        "                # keep all labels above threshold\n",
        "                kept = [lab for lab, sc in zip(res.get(\"labels\", []), res.get(\"scores\", [])) if sc >= ZS_SCORE_THRESH]\n",
        "                topics_multi.append(kept if kept else [\"unclassified\"])\n",
        "            else:\n",
        "                topics_multi.append([res.get(\"labels\", [\"unclassified\"])[0]])\n",
        "\n",
        "    # Store best single topic and list of topics\n",
        "    best_single = [ts[0] if ts else \"unclassified\" for ts in topics_multi]\n",
        "    topic_col.loc[idxs] = best_single\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"Topic\"] = topic_col.fillna(\"unclassified\")\n",
        "    df[\"Topics_All\"] = None\n",
        "    # Insert lists; pandas will store as object (JSON-serializable on save if needed)\n",
        "    for k, ix in enumerate(idxs):\n",
        "        df.at[ix, \"Topics_All\"] = topics_multi[k]\n",
        "\n",
        "    df.to_csv(TOPIC_CSV, index=False)\n",
        "    return df\n",
        "\n",
        "# ----------------------\n",
        "# 6) SUMMARIZATION (NEGATIVE REVIEWS PER TOPIC)\n",
        "# ----------------------\n",
        "def summarize_negative_by_topic(df: pd.DataFrame) -> None:\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    summarizer = pipeline(\"summarization\", model=MODEL_SUM, device=device)\n",
        "\n",
        "    neg = df[df[\"Hybrid_Sentiment_Label\"] == \"NEGATIVE\"].copy()\n",
        "    topics = sorted(t for t in neg[\"Topic\"].dropna().unique() if t != \"unclassified\")\n",
        "\n",
        "    with open(NEG_SUMMARY_TXT, \"w\") as f:\n",
        "        for t in topics:\n",
        "            reviews = (\n",
        "                neg.loc[neg[\"Topic\"] == t, \"Review\"]\n",
        "                .dropna()\n",
        "                .astype(str)\n",
        "                .tolist()\n",
        "            )[:MAX_REVIEWS_PER_TOPIC_FOR_SUMMARY]\n",
        "\n",
        "            if len(reviews) < 3:\n",
        "                f.write(f\"Topic: {t}\\nSummary: Not enough data to summarize.\\n\\n\")\n",
        "                continue\n",
        "\n",
        "            text = \" \".join(reviews)[:2048]  # truncate safety\n",
        "            try:\n",
        "                s = summarizer(text, max_length=SUMMARY_MAX_LEN, min_length=SUMMARY_MIN_LEN, do_sample=False)\n",
        "                summary = s[0][\"summary_text\"]\n",
        "            except Exception:\n",
        "                summary = \"Summarization failed.\"\n",
        "            f.write(f\"Topic: {t}\\nSummary: {summary}\\n\\n\")\n",
        "\n",
        "# ----------------------\n",
        "# 7) N-GRAM MINING (NEGATIVE/NEUTRAL)\n",
        "# ----------------------\n",
        "def mine_ngrams(df: pd.DataFrame, ngram_range=(2, 3), top_k=50) -> pd.DataFrame:\n",
        "    focus = df[df[\"Hybrid_Sentiment_Label\"].isin([\"NEGATIVE\", \"NEUTRAL\"])].copy()\n",
        "    if focus.empty:\n",
        "        pd.DataFrame({\"Feature\": [], \"Frequency\": []}).to_csv(NGRAMS_CSV, index=False)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    vec = CountVectorizer(\n",
        "        ngram_range=ngram_range,\n",
        "        stop_words=\"english\",\n",
        "        max_features=top_k\n",
        "    )\n",
        "    X = vec.fit_transform(focus[\"Review\"].astype(str))\n",
        "    feats = vec.get_feature_names_out()\n",
        "    counts = X.sum(axis=0).A1\n",
        "    freq = pd.DataFrame({\"Feature\": feats, \"Frequency\": counts}).sort_values(\"Frequency\", ascending=False)\n",
        "    freq.to_csv(NGRAMS_CSV, index=False)\n",
        "    return freq\n",
        "\n",
        "# ----------------------\n",
        "# 8) FEATURE-REQUEST FLAGS (REGEX)\n",
        "# ----------------------\n",
        "FEATURE_PATTERNS = [\n",
        "    r\"\\bi wish\\b\",\n",
        "    r\"\\bwould love\\b\",\n",
        "    r\"\\bplease add\\b\",\n",
        "    r\"\\bwould be great\\b\",\n",
        "    r\"\\bit would be nice\\b\",\n",
        "    r\"\\bplease include\\b\",\n",
        "    r\"\\bhope you can\\b\",\n",
        "    r\"\\bcan you add\\b\",\n",
        "    r\"\\bmissing\\b\",\n",
        "    r\"\\bshould have\\b\",\n",
        "    r\"\\bmust have\\b\",\n",
        "    r\"\\bneed(?:s)?\\s+(?:feature|option|support)\\b\",\n",
        "    r\"\\badd\\s+(?:support|option|feature)\\b\",\n",
        "    r\"\\bwould be helpful\\b\",\n",
        "    r\"\\bshould include\\b\",\n",
        "    r\"\\bcould use\\b\",\n",
        "]\n",
        "\n",
        "def flag_feature_requests(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    pat = re.compile(\"|\".join(FEATURE_PATTERNS), flags=re.IGNORECASE)\n",
        "    out = df.copy()\n",
        "    out[\"Feature_Request\"] = out[\"Review\"].fillna(\"\").astype(str).apply(lambda x: bool(pat.search(x)))\n",
        "    out[out[\"Feature_Request\"]].to_csv(FEATURE_REQUESTS_CSV, index=False)\n",
        "    return out\n",
        "\n",
        "# ----------------------\n",
        "# 9) APP-LEVEL SUMMARY\n",
        "# ----------------------\n",
        "def build_app_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    d = df.copy()\n",
        "    d[\"App_Name\"] = d[\"App_Name\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # sentiment distribution per app (normalized)\n",
        "    sent = (\n",
        "        d.groupby(\"App_Name\")[\"Hybrid_Sentiment_Label\"]\n",
        "        .value_counts(normalize=True)\n",
        "        .unstack(fill_value=0)\n",
        "        .rename(columns={\n",
        "            \"POSITIVE\": \"Positive(%)\", \"NEGATIVE\": \"Negative(%)\", \"NEUTRAL\": \"Neutral(%)\"\n",
        "        })\n",
        "    )\n",
        "\n",
        "    # total reviews per app\n",
        "    counts = d[\"App_Name\"].value_counts().rename(\"Total_Reviews\").to_frame()\n",
        "\n",
        "    # most common topic per app (if any topics)\n",
        "    if \"Topic\" in d.columns:\n",
        "        top_topic = (\n",
        "            d.groupby(\"App_Name\")[\"Topic\"]\n",
        "            .agg(lambda x: x.value_counts().idxmax() if not x.dropna().empty else \"unclassified\")\n",
        "            .rename(\"Top_Topic\")\n",
        "        )\n",
        "    else:\n",
        "        top_topic = pd.Series(dtype=object, name=\"Top_Topic\")\n",
        "\n",
        "    summary = sent.merge(counts, left_index=True, right_index=True, how=\"left\")\n",
        "    if not top_topic.empty:\n",
        "        summary = summary.merge(top_topic, left_index=True, right_index=True, how=\"left\")\n",
        "\n",
        "    for col in [\"Positive(%)\", \"Negative(%)\", \"Neutral(%)\"]:\n",
        "        if col in summary.columns:\n",
        "            summary[col] = (summary[col] * 100).round(1)\n",
        "\n",
        "    summary = summary.sort_values(\"Total_Reviews\", ascending=False).reset_index().rename(columns={\"index\": \"App_Name\"})\n",
        "    summary.to_csv(APP_SUMMARY_CSV, index=False)\n",
        "    return summary\n",
        "\n",
        "# ----------------------\n",
        "# MAIN\n",
        "# ----------------------\n",
        "def main():\n",
        "    print(\"üì• Ingesting & standardizing ...\")\n",
        "    df = ingest_and_standardize()\n",
        "\n",
        "    print(\"üßπ Cleaning ...\")\n",
        "    df = clean(df)\n",
        "\n",
        "    print(\"üí¨ Running ML sentiment (batched) ...\")\n",
        "    df = run_ml_sentiment(df)\n",
        "\n",
        "    print(\"üîÄ Applying hybrid sentiment fallback ...\")\n",
        "    df = apply_hybrid_sentiment(df)\n",
        "\n",
        "    print(\"üè∑Ô∏è Zero-shot topic tagging (batched) ...\")\n",
        "    df = zero_shot_topics(df)\n",
        "\n",
        "    print(\"üßæ Summarizing negative reviews per topic ...\")\n",
        "    summarize_negative_by_topic(df)\n",
        "\n",
        "    print(\"üß© Mining top n-grams (neg/neutral) ...\")\n",
        "    _ = mine_ngrams(df)\n",
        "\n",
        "    print(\"‚ú® Flagging feature requests ...\")\n",
        "    df = flag_feature_requests(df)\n",
        "\n",
        "    print(\"üìä Building app-level summary ...\")\n",
        "    _ = build_app_summary(df)\n",
        "\n",
        "    print(\"\\n‚úÖ Done! Artifacts saved in:\", OUT_DIR.resolve())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}