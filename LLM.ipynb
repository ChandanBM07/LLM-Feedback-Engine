{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jrUj6wvrkFU"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "google_df = pd.read_csv('Play Store Data.csv')\n",
        "apple_df = pd.read_csv('Apple_Store_Reviews.csv')\n",
        "\n",
        "# Add a source column to track origin\n",
        "google_df['Source'] = 'Google Play Store'\n",
        "apple_df['Source'] = 'Apple App Store'\n",
        "\n",
        "# Standardize column names (example: rename if needed)\n",
        "# Adjust these as per your real column names\n",
        "google_df.rename(columns={'App': 'App_Name', 'Rating': 'App_Rating'}, inplace=True)\n",
        "apple_df.rename(columns={'track_name': 'App_Name', 'user_rating': 'App_Rating'}, inplace=True)\n",
        "\n",
        "# Select common columns only (adjust as needed)\n",
        "common_cols = ['App_Name', 'App_Rating', 'Source']\n",
        "google_common = google_df[common_cols]\n",
        "apple_common = apple_df[common_cols]\n",
        "\n",
        "# Concatenate the datasets\n",
        "combined_df = pd.concat([google_common, apple_common], ignore_index=True)\n",
        "\n",
        "# Check result\n",
        "print(\"Combined dataset shape:\", combined_df.shape)\n",
        "print(combined_df.head())\n",
        "\n",
        "# Save to new CSV\n",
        "combined_df.to_csv('combined_app_reviews.csv', index=False)\n",
        "print(\"✅ Saved as combined_app_reviews.csv\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "google_df = pd.read_csv('Play Store Data.csv')\n",
        "apple_df = pd.read_csv('Apple_Store_Reviews.csv')\n",
        "\n",
        "# Standardize column names\n",
        "google_df.rename(columns={'App': 'App_Name', 'Rating': 'App_Rating'}, inplace=True)\n",
        "apple_df.rename(columns={'App': 'App_Name', 'Rating': 'App_Rating'}, inplace=True)\n",
        "\n",
        "# Add source column to each\n",
        "google_df['Source'] = 'Google Play Store'\n",
        "apple_df['Source'] = 'Apple App Store'\n",
        "\n",
        "# Select only common columns\n",
        "common_cols = ['App_Name', 'App_Rating', 'Source']\n",
        "google_common = google_df[common_cols]\n",
        "apple_common = apple_df[common_cols]\n",
        "\n",
        "# Combine datasets\n",
        "combined_df = pd.concat([google_common, apple_common], ignore_index=True)\n",
        "\n",
        "# Save combined data\n",
        "combined_df.to_csv('combined_app_reviews.csv', index=False)\n",
        "print(\"✅ Combined dataset saved as 'combined_app_reviews.csv'\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "google_df = pd.read_csv('Play Store Data.csv')\n",
        "apple_df = pd.read_csv('Apple_Store_Reviews.csv')\n",
        "\n",
        "# Inspect available columns\n",
        "print(\"Google columns:\", google_df.columns)\n",
        "print(\"Apple columns:\", apple_df.columns)\n",
        "\n",
        "# Adjust column names to standardize\n",
        "google_df.rename(columns={'App': 'App_Name', 'Rating': 'App_Rating', 'Review': 'Review','Category':'Category'}, inplace=True)\n",
        "apple_df.rename(columns={'App': 'App_Name', 'Rating': 'App_Rating', 'Review_Text': 'Review','Category':'Category'}, inplace=True)\n",
        "\n",
        "# Add source column to each\n",
        "google_df['Source'] = 'Google Play Store'\n",
        "apple_df['Source'] = 'Apple App Store'\n",
        "\n",
        "# Select only the common + important columns\n",
        "common_cols = ['App_Name', 'App_Rating', 'Review','Category','Source']\n",
        "google_common = google_df[common_cols]\n",
        "apple_common = apple_df[common_cols]\n",
        "\n",
        "# Combine datasets\n",
        "combined_df = pd.concat([google_common, apple_common], ignore_index=True)\n",
        "\n",
        "# Save combined data\n",
        "combined_df.to_csv('combined_app_reviews.csv', index=False)\n",
        "print(\"✅ Combined dataset saved as 'combined_app_reviews.csv'\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "google_df = pd.read_csv('Play Store Data.csv')\n",
        "apple_df = pd.read_csv('Apple_Store_Reviews.csv')\n",
        "\n",
        "# Inspect available columns\n",
        "print(\"Google columns:\", google_df.columns)\n",
        "print(\"Apple columns:\", apple_df.columns)\n",
        "\n",
        "# Adjust column names to standardize\n",
        "google_df.rename(columns={\n",
        "    'App': 'App_Name',\n",
        "    'Rating': 'App_Rating',\n",
        "    'Review': 'Review',\n",
        "    'Category': 'Category'\n",
        "}, inplace=True)\n",
        "\n",
        "apple_df.rename(columns={\n",
        "    'App': 'App_Name',\n",
        "    'Rating': 'App_Rating',\n",
        "    'Review_Text': 'Review',  # Assuming Apple uses 'Review_Text'\n",
        "    'Category': 'Category'\n",
        "}, inplace=True)\n",
        "\n",
        "# Add source column to each\n",
        "google_df['Source'] = 'Google Play Store'\n",
        "apple_df['Source'] = 'Apple App Store'\n",
        "\n",
        "# Select only the common + important columns\n",
        "common_cols = ['App_Name', 'App_Rating', 'Review', 'Category', 'Source']\n",
        "google_common = google_df[common_cols]\n",
        "apple_common = apple_df[common_cols]\n",
        "\n",
        "# Combine datasets\n",
        "combined_df = pd.concat([google_common, apple_common], ignore_index=True)\n",
        "\n",
        "# Save combined data\n",
        "combined_df.to_csv('combined_app_reviews.csv', index=False)\n",
        "print(\"✅ Combined dataset saved as 'combined_app_reviews.csv'\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "google_df = pd.read_csv('Play Store Data.csv')\n",
        "apple_df = pd.read_csv('Apple_Store_Reviews.csv')\n",
        "\n",
        "# Inspect columns (you already printed this)\n",
        "# Google: no real 'Review' column, only 'Reviews' (count)\n",
        "# Apple: has 'Review_Text'\n",
        "\n",
        "# Rename + standardize\n",
        "google_df.rename(columns={\n",
        "    'App': 'App_Name',\n",
        "    'Rating': 'App_Rating',\n",
        "    'Category': 'Category'\n",
        "}, inplace=True)\n",
        "\n",
        "apple_df.rename(columns={\n",
        "    'App_Name': 'App_Name',\n",
        "    'Rating': 'App_Rating',\n",
        "    'Review_Text': 'Review',\n",
        "    'Category': 'Category'\n",
        "}, inplace=True)\n",
        "\n",
        "# Add placeholder 'Review' column for Google\n",
        "google_df['Review'] = 'No Review Available'\n",
        "\n",
        "# Add source column\n",
        "google_df['Source'] = 'Google Play Store'\n",
        "apple_df['Source'] = 'Apple App Store'\n",
        "\n",
        "# Select relevant columns\n",
        "common_cols = ['App_Name', 'App_Rating', 'Review', 'Category', 'Source']\n",
        "google_common = google_df[common_cols]\n",
        "apple_common = apple_df[common_cols]\n",
        "\n",
        "# Combine datasets\n",
        "combined_df = pd.concat([google_common, apple_common], ignore_index=True)\n",
        "\n",
        "# Save combined file\n",
        "combined_df.to_csv('combined_app_reviews1.csv', index=False)\n",
        "print(\"✅ Combined dataset saved as 'combined_app_reviews1.csv'\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load combined dataset\n",
        "df = pd.read_csv('combined_app_reviews.csv')\n",
        "\n",
        "print(\"Original shape:\", df.shape)\n",
        "\n",
        "# 1️⃣ Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "print(\"After removing duplicates:\", df.shape)\n",
        "\n",
        "# 2️⃣ Handle missing values\n",
        "# For ratings → fill with median (per app if possible)\n",
        "if 'App_Rating' in df.columns:\n",
        "    df['App_Rating'] = pd.to_numeric(df['App_Rating'], errors='coerce')  # ensure numeric\n",
        "    df['App_Rating'] = df.groupby('App_Name')['App_Rating'].transform(lambda x: x.fillna(x.median()))\n",
        "    # if still missing, fill with overall median\n",
        "    df['App_Rating'] = df['App_Rating'].fillna(df['App_Rating'].median())\n",
        "\n",
        "# For Review → fill with \"No Review\"\n",
        "df['Review'] = df['Review'].fillna('No Review').astype(str)\n",
        "\n",
        "# For Category → fill with 'Unknown'\n",
        "df['Category'] = df['Category'].fillna('Unknown')\n",
        "\n",
        "# 3️⃣ Trim whitespace + lowercase text columns\n",
        "text_cols = ['App_Name', 'Review', 'Category', 'Source']\n",
        "for col in text_cols:\n",
        "    df[col] = df[col].astype(str).str.strip().str.lower()\n",
        "\n",
        "# 4️⃣ Remove outliers (App_Rating > 5 or < 1)\n",
        "df = df[(df['App_Rating'] >= 1) & (df['App_Rating'] <= 5)]\n",
        "print(\"After removing rating outliers:\", df.shape)\n",
        "\n",
        "# 5️⃣ Final check for any remaining missing values\n",
        "print(\"Missing values summary:\\n\", df.isnull().sum())\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv('super_cleaned_app_reviews.csv', index=False)\n",
        "print(\"✅ Super cleaned dataset saved as 'super_cleaned_app_reviews.csv'\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load your super cleaned dataset\n",
        "df = pd.read_csv('super_cleaned_app_reviews.csv')\n",
        "\n",
        "# Function to compute sentiment polarity and label\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(str(text))\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.1:\n",
        "        label = 'Positive'\n",
        "    elif polarity < -0.1:\n",
        "        label = 'Negative'\n",
        "    else:\n",
        "        label = 'Neutral'\n",
        "    return pd.Series([polarity, label])\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df[['Sentiment_Polarity', 'Sentiment_Label']] = df['Review'].apply(get_sentiment)\n",
        "\n",
        "# ✅ Group by Category and summarize\n",
        "category_summary = df.groupby('Category')['Sentiment_Label'].value_counts().unstack().fillna(0)\n",
        "print(\"\\n📊 Sentiment Summary by Category:\")\n",
        "print(category_summary)\n",
        "\n",
        "# ✅ Group by App_Name and summarize\n",
        "app_summary = df.groupby('App_Name')['Sentiment_Label'].value_counts().unstack().fillna(0)\n",
        "print(\"\\n📊 Sentiment Summary by App:\")\n",
        "print(app_summary)\n",
        "\n",
        "# ✅ Group by Ratings and summarize (if App_Rating exists)\n",
        "if 'App_Rating' in df.columns:\n",
        "    rating_summary = df.groupby('App_Rating')['Sentiment_Label'].value_counts().unstack().fillna(0)\n",
        "    print(\"\\n📊 Sentiment Summary by App Rating:\")\n",
        "    print(rating_summary)\n",
        "else:\n",
        "    print(\"\\n⚠️ No 'App_Rating' column found.\")\n",
        "\n",
        "# Save enriched data\n",
        "df.to_csv('sentiment_labeled_app_reviews.csv', index=False)\n",
        "print(\"\\n✅ Saved: sentiment_labeled_app_reviews.csv\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Load your cleaned dataset\n",
        "df = pd.read_csv(\"super_cleaned_app_reviews.csv\")\n",
        "\n",
        "# ✅ Initialize Hugging Face pipelines\n",
        "theme_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# ✅ Define your custom theme labels\n",
        "theme_labels = [\"UI/UX\", \"Performance\", \"Bugs\", \"Features\", \"Customer Support\", \"Pricing\", \"Functionality\"]\n",
        "\n",
        "# ✅ Progress bar setup\n",
        "tqdm.pandas()\n",
        "\n",
        "# Step 1: Classify themes\n",
        "def classify_theme(text):\n",
        "    try:\n",
        "        result = theme_classifier(text, candidate_labels=theme_labels)\n",
        "        return result[\"labels\"][0], result[\"scores\"][0]\n",
        "    except:\n",
        "        return \"Unknown\", 0.0\n",
        "\n",
        "df[[\"Theme\", \"Theme_Confidence\"]] = df[\"Review\"].progress_apply(lambda x: pd.Series(classify_theme(str(x))))\n",
        "\n",
        "# Step 2: Summarize category-wise feedback\n",
        "summaries = {}\n",
        "for category in df[\"Category\"].unique():\n",
        "    subset = df[df[\"Category\"] == category][\"Review\"].dropna().astype(str).tolist()\n",
        "    text_chunk = \" \".join(subset[:5])  # summarize first 5 reviews per category\n",
        "    if len(text_chunk) > 50:\n",
        "        try:\n",
        "            summary = summarizer(text_chunk, max_length=80, min_length=25, do_sample=False)[0][\"summary_text\"]\n",
        "            summaries[category] = summary\n",
        "        except:\n",
        "            summaries[category] = \"Summary generation failed.\"\n",
        "    else:\n",
        "        summaries[category] = \"Too little text to summarize.\"\n",
        "\n",
        "# Map summaries back\n",
        "df[\"Category_Summary\"] = df[\"Category\"].map(summaries)\n",
        "\n",
        "# ✅ Save enhanced insights\n",
        "output_path = \"llm_enriched_feedback.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"✅ Saved: {output_path}\")\n",
        "\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Replace this with your actual token\n",
        "api_token = \" \"#add your hugging face token\n",
        "\n",
        "# Load your CSV with reviews\n",
        "df = pd.read_csv(\"llm_enriched_feedback.csv\")\n",
        "df = df.dropna(subset=[\"Review\"])  # Ensure no nulls\n",
        "\n",
        "# Setup Hugging Face API endpoint and headers\n",
        "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-mnli\"\n",
        "headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
        "\n",
        "# Define labels for classification\n",
        "labels = [\"Bug Report\", \"Feature Request\", \"Praise\", \"Complaint\"]\n",
        "\n",
        "def classify(text):\n",
        "    payload = {\n",
        "        \"inputs\": text,\n",
        "        \"parameters\": {\"candidate_labels\": labels}\n",
        "    }\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result[\"labels\"][0], result[\"scores\"][0]\n",
        "    else:\n",
        "        return \"Unknown\", 0.0\n",
        "\n",
        "# Run classification and store results\n",
        "tqdm.pandas()\n",
        "df[[\"Category\", \"Confidence\"]] = df[\"Review\"].progress_apply(lambda x: pd.Series(classify(str(x))))\n",
        "\n",
        "# Save to new file\n",
        "df.to_csv(\"feedback_with_categories_api.csv\", index=False)\n",
        "print(\"✅ Saved: feedback_with_categories_api.csv\")\n",
        "\n",
        "\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=\" \")  # ✅ required for private or rate-limited models, add your token\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load enriched data\n",
        "df = pd.read_csv(\"llm_enriched_feedback.csv\")\n",
        "\n",
        "# Clean column names (if needed)\n",
        "df.columns = [col.strip() for col in df.columns]\n",
        "\n",
        "# Plot: Sentiment Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x=\"sentiment_Label\", palette=\"Set2\")\n",
        "plt.title(\"LLM-Powered Sentiment Distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot: Sentiment by Category\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x=\"Category\", hue=\"Sentiment_Label\", palette=\"coolwarm\")\n",
        "plt.title(\"Sentiment by App Category\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Load your file\n",
        "df = pd.read_csv(\"llm_enriched_feedback.csv\")\n",
        "\n",
        "# Initialize sentiment model\n",
        "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Add sentiment\n",
        "tqdm.pandas()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    try:\n",
        "        result = sentiment_model(text[:512])[0]  # limit input length\n",
        "        return result[\"label\"], result[\"score\"]\n",
        "    except:\n",
        "        return \"UNKNOWN\", 0\n",
        "\n",
        "df[[\"Sentiment_Label\", \"Sentiment_Score\"]] = df[\"Review\"].astype(str).progress_apply(\n",
        "    lambda x: pd.Series(get_sentiment(x))\n",
        ")\n",
        "\n",
        "# Save updated file\n",
        "df.to_csv(\"llm_enriched_feedback_with_sentiment.csv\", index=False)\n",
        "print(\"✅ Added Sentiment_Label and saved as 'llm_enriched_feedback_with_sentiment.csv'\")\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(\"llm_enriched_feedback_with_sentiment.csv\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x=\"Sentiment_Label\", palette=\"Set2\")\n",
        "plt.title(\"LLM-Powered Sentiment Distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"llm_enriched_feedback.csv\")\n",
        "\n",
        "# Clean placeholder reviews\n",
        "df[\"Review\"] = df[\"Review\"].fillna(\"\").astype(str)\n",
        "df[\"Review_Lower\"] = df[\"Review\"].str.lower()\n",
        "\n",
        "# Define reviews with no usable content\n",
        "placeholder_phrases = [\n",
        "    \"no review available\", \"no reviews available\", \"no comment\", \"n/a\", \"none\", \"\"\n",
        "]\n",
        "df[\"Is_Empty_Review\"] = df[\"Review_Lower\"].apply(lambda x: any(p in x for p in placeholder_phrases))\n",
        "\n",
        "# Load Hugging Face sentiment model\n",
        "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Define function: use text if available, else use rating\n",
        "def hybrid_sentiment(row):\n",
        "    if not row[\"Is_Empty_Review\"]:\n",
        "        try:\n",
        "            result = sentiment_model(row[\"Review\"][:512])[0]  # Truncate long reviews\n",
        "            return result[\"label\"], float(result[\"score\"])\n",
        "        except:\n",
        "            return \"NEUTRAL\", 0.5\n",
        "    else:\n",
        "        rating = row.get(\"App_Rating\", 0)\n",
        "        if rating >= 4:\n",
        "            return \"POSITIVE\", 1.0\n",
        "        elif rating <= 2:\n",
        "            return \"NEGATIVE\", 1.0\n",
        "        else:\n",
        "            return \"NEUTRAL\", 1.0\n",
        "\n",
        "# Apply hybrid sentiment logic\n",
        "print(\"🔍 Applying hybrid sentiment classification...\")\n",
        "df[[\"Hybrid_Sentiment_Label\", \"Hybrid_Sentiment_Confidence\"]] = df.apply(hybrid_sentiment, axis=1, result_type=\"expand\")\n",
        "\n",
        "# Save updated dataset\n",
        "df.drop(columns=[\"Review_Lower\", \"Is_Empty_Review\"], inplace=True)\n",
        "df.to_csv(\"hybrid_sentiment_output.csv\", index=False)\n",
        "print(\"✅ Saved: hybrid_sentiment_output.csv\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load and clean\n",
        "df = pd.read_csv(\"llm_enriched_feedback.csv\")\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "print(\"🧾 Columns:\", df.columns.tolist())\n",
        "\n",
        "# Plot based on correct column name\n",
        "if 'sentiment' in df.columns:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.countplot(data=df, x=\"sentiment\", palette=\"Set2\")\n",
        "    plt.title(\"LLM-Powered Sentiment Distribution\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"❌ Column 'sentiment' not found.\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"hybrid_sentiment_output.csv\")\n",
        "\n",
        "# Check available sentiment labels\n",
        "print(df[\"Sentiment_Label\"].value_counts())\n",
        "\n",
        "# Plot Sentiment Distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x=\"Sentiment_Label\", palette=\"Set2\")\n",
        "plt.title(\"Hybrid LLM Sentiment Distribution\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Review Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Sentiment per App\n",
        "top_apps = df['App_Name'].value_counts().head(5).index.tolist()\n",
        "subset = df[df['App_Name'].isin(top_apps)]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=subset, x=\"App_Name\", hue=\"Sentiment_Label\", palette=\"coolwarm\")\n",
        "plt.title(\"Sentiment Breakdown for Top 5 Apps\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Assuming 'Review_Date' column exists and is datetime type\n",
        "subset['Review_Date'] = pd.to_datetime(subset['Review_Date'])\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.lineplot(data=subset, x='Review_Date', hue='Sentiment_Label', estimator='count', style='App_Name')\n",
        "plt.title('Sentiment Trend Over Time for Top 5 Apps')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.legend(title='Sentiment & App')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Split hybrid data into Apple and Play Store subsets\n",
        "apple_hybrid = hybrid_df[hybrid_df['Source'] == 'Apple']\n",
        "playstore_hybrid = hybrid_df[hybrid_df['Source'] == 'PlayStore']\n",
        "\n",
        "# Merge Apple reviews on Review_ID\n",
        "apple_merged = apple_hybrid.merge(apple_subset, on=['Review_ID', 'App_Name'], how='left')\n",
        "\n",
        "# Merge Play Store reviews on App_Name only\n",
        "playstore_merged = playstore_hybrid.merge(playstore_subset, on='App_Name', how='left')\n",
        "\n",
        "# Combine back the two parts\n",
        "final_df = pd.concat([apple_merged, playstore_merged], ignore_index=True)\n",
        "\n",
        "# Save the updated file\n",
        "final_df.to_csv('hybrid_sentiment_output_with_date.csv', index=False)\n",
        "\n",
        "print(\"Review dates merged successfully!\")\n",
        "\n",
        "\n",
        "# Identify source\n",
        "hybrid_df['Source'] = hybrid_df['Review_ID'].apply(lambda x: 'Apple' if pd.notna(x) else 'PlayStore')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load your files\n",
        "apple_df = pd.read_csv('Apple_Store_Reviews.csv')        # Apple source file\n",
        "playstore_df = pd.read_csv('Play Store Data.csv')     # Play Store source file\n",
        "hybrid_df = pd.read_csv('hybrid_sentiment_output.csv')   # Hybrid sentiment output without date\n",
        "\n",
        "# Step 2: Identify source in hybrid data (assuming Apple reviews have Review_ID)\n",
        "hybrid_df['Source'] = hybrid_df['Review_ID'].apply(lambda x: 'Apple' if pd.notna(x) else 'PlayStore')\n",
        "\n",
        "# Step 3: Prepare source subsets with relevant columns\n",
        "apple_subset = apple_df[['Review_ID', 'App_Name', 'review date']]\n",
        "playstore_subset = playstore_df[['App_Name', 'review date']]\n",
        "\n",
        "# Step 4: Split hybrid data by source\n",
        "apple_hybrid = hybrid_df[hybrid_df['Source'] == 'Apple']\n",
        "playstore_hybrid = hybrid_df[hybrid_df['Source'] == 'PlayStore']\n",
        "\n",
        "# Step 5: Merge Apple reviews on Review_ID and App_Name\n",
        "apple_merged = apple_hybrid.merge(apple_subset, on=['Review_ID', 'App_Name'], how='left')\n",
        "\n",
        "# Step 6: Merge Play Store reviews on App_Name only\n",
        "playstore_merged = playstore_hybrid.merge(playstore_subset, on='App_Name', how='left')\n",
        "\n",
        "# Step 7: Combine both back together\n",
        "final_df = pd.concat([apple_merged, playstore_merged], ignore_index=True)\n",
        "\n",
        "# Step 8: Save to new CSV\n",
        "final_df.to_csv('hybrid_sentiment_output_with_date.csv', index=False)\n",
        "\n",
        "print(\"Review dates merged and saved successfully!\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load data (make sure your source CSV has 'review date' column)\n",
        "df = pd.read_csv(\"llm_enriched_feedback.csv\")\n",
        "\n",
        "# Clean placeholder reviews\n",
        "df[\"Review\"] = df[\"Review\"].fillna(\"\").astype(str)\n",
        "df[\"Review_Lower\"] = df[\"Review\"].str.lower()\n",
        "\n",
        "placeholder_phrases = [\n",
        "    \"no review available\", \"no reviews available\", \"no comment\", \"n/a\", \"none\", \"\"\n",
        "]\n",
        "df[\"Is_Empty_Review\"] = df[\"Review_Lower\"].apply(lambda x: any(p in x for p in placeholder_phrases))\n",
        "\n",
        "# Load Hugging Face sentiment model\n",
        "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "def hybrid_sentiment(row):\n",
        "    if not row[\"Is_Empty_Review\"]:\n",
        "        try:\n",
        "            result = sentiment_model(row[\"Review\"][:512])[0]\n",
        "            return result[\"label\"], float(result[\"score\"])\n",
        "        except:\n",
        "            return \"NEUTRAL\", 0.5\n",
        "    else:\n",
        "        rating = row.get(\"App_Rating\", 0)\n",
        "        if rating >= 4:\n",
        "            return \"POSITIVE\", 1.0\n",
        "        elif rating <= 2:\n",
        "            return \"NEGATIVE\", 1.0\n",
        "        else:\n",
        "            return \"NEUTRAL\", 1.0\n",
        "\n",
        "print(\"🔍 Applying hybrid sentiment classification...\")\n",
        "df[[\"Hybrid_Sentiment_Label\", \"Hybrid_Sentiment_Confidence\"]] = df.apply(hybrid_sentiment, axis=1, result_type=\"expand\")\n",
        "\n",
        "# Clean up helper columns\n",
        "df.drop(columns=[\"Review_Lower\", \"Is_Empty_Review\"], inplace=True)\n",
        "\n",
        "# Make sure 'review date' column is still there before saving\n",
        "print(df.columns)  # Just to confirm 'review date' exists\n",
        "\n",
        "# Save to CSV including review date\n",
        "df.to_csv(\"hybrid_sentiment_output_with_date.csv\", index=False)\n",
        "print(\"✅ Saved: hybrid_sentiment_output_with_date.csv with review date included\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load main data\n",
        "df = pd.read_csv(\"llm_enriched_feedback.csv\")\n",
        "\n",
        "# Load source files with review dates\n",
        "apple_df = pd.read_csv(\"Apple_Store_Reviews.csv\")\n",
        "playstore_df = pd.read_csv(\"Play Store Data.csv\")\n",
        "\n",
        "# Identify source by Review_ID presence\n",
        "if 'Review_ID' in df.columns:\n",
        "    df['Source'] = df['Review_ID'].apply(lambda x: 'Apple' if pd.notna(x) else 'PlayStore')\n",
        "else:\n",
        "    df['Source'] = 'PlayStore'  # or use app name check if needed\n",
        "\n",
        "# Split and merge\n",
        "apple_rows = df[df['Source'] == 'Apple']\n",
        "playstore_rows = df[df['Source'] == 'PlayStore']\n",
        "\n",
        "apple_merged = apple_rows.merge(\n",
        "    apple_df[['Review_ID', 'App_Name', 'Review_Date']],\n",
        "    on=['Review_ID', 'App_Name'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "playstore_merged = playstore_rows.merge(\n",
        "    playstore_df[['App_Name', 'Review_Date']],\n",
        "    on='App_Name',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "df = pd.concat([apple_merged, playstore_merged], ignore_index=True)\n",
        "\n",
        "# Now continue your existing processing\n",
        "\n",
        "df[\"Review\"] = df[\"Review\"].fillna(\"\").astype(str)\n",
        "df[\"Review_Lower\"] = df[\"Review\"].str.lower()\n",
        "\n",
        "placeholder_phrases = [\n",
        "    \"no review available\", \"no reviews available\", \"no comment\", \"n/a\", \"none\", \"\"\n",
        "]\n",
        "df[\"Is_Empty_Review\"] = df[\"Review_Lower\"].apply(lambda x: any(p in x for p in placeholder_phrases))\n",
        "\n",
        "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "def hybrid_sentiment(row):\n",
        "    if not row[\"Is_Empty_Review\"]:\n",
        "        try:\n",
        "            result = sentiment_model(row[\"Review\"][:512])[0]\n",
        "            return result[\"label\"], float(result[\"score\"])\n",
        "        except:\n",
        "            return \"NEUTRAL\", 0.5\n",
        "    else:\n",
        "        rating = row.get(\"App_Rating\", 0)\n",
        "        if rating >= 4:\n",
        "            return \"POSITIVE\", 1.0\n",
        "        elif rating <= 2:\n",
        "            return \"NEGATIVE\", 1.0\n",
        "        else:\n",
        "            return \"NEUTRAL\", 1.0\n",
        "\n",
        "print(\"🔍 Applying hybrid sentiment classification...\")\n",
        "df[[\"Hybrid_Sentiment_Label\", \"Hybrid_Sentiment_Confidence\"]] = df.apply(hybrid_sentiment, axis=1, result_type=\"expand\")\n",
        "\n",
        "df.drop(columns=[\"Review_Lower\", \"Is_Empty_Review\", \"Source\"], inplace=True)\n",
        "\n",
        "df.to_csv(\"hybrid_sentiment_output_with_date.csv\", index=False)\n",
        "print(\"✅ Saved with review date included.\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load your main review file\n",
        "df = pd.read_csv(\"llm_enriched_feedback.csv\")\n",
        "\n",
        "# Load source files with review dates\n",
        "apple_df = pd.read_csv(\"Apple_Store_Reviews.csv\")\n",
        "playstore_df = pd.read_csv(\"Play Store Data.csv\")\n",
        "\n",
        "# Step 1: Identify Source\n",
        "df['Source'] = df['Review_ID'].apply(lambda x: 'Apple' if pd.notna(x) else 'PlayStore')\n",
        "\n",
        "# Step 2: Split df based on Source\n",
        "apple_rows = df[df['Source'] == 'Apple']\n",
        "playstore_rows = df[df['Source'] == 'PlayStore']\n",
        "\n",
        "# Step 3: Merge Apple data on Review_ID + App_Name\n",
        "apple_merged = apple_rows.merge(\n",
        "    apple_df[['Review_ID', 'App_Name', 'review date']],\n",
        "    on=['Review_ID', 'App_Name'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Step 4: Merge Play Store data on App_Name only\n",
        "playstore_merged = playstore_rows.merge(\n",
        "    playstore_df[['App_Name', 'review date']],\n",
        "    on='App_Name',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Step 5: Combine both\n",
        "df = pd.concat([apple_merged, playstore_merged], ignore_index=True)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load enriched feedback (no Review_ID)\n",
        "df = pd.read_csv(\"llm_enriched_feedback.csv\")\n",
        "\n",
        "# Load Play Store source data (has App_Name + review date)\n",
        "playstore_df = pd.read_csv(\"Play Store Data.csv\")\n",
        "\n",
        "# Merge to bring in 'review date' based on App_Name\n",
        "df = df.merge(\n",
        "    playstore_df[['App_Name', 'Review_Date']],\n",
        "    on='App_Name',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Optional: check how many rows got matched\n",
        "matched = df['Review_Date'].notna().sum()\n",
        "missing = df['Review_Date'].isna().sum()\n",
        "print(f\"✅ Review date matched for {matched} rows; missing in {missing} rows.\")\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Focus on actionable feedback (optional: only negative or neutral)\n",
        "feedback_df = df[df['Sentiment_Label'].isin(['NEGATIVE', 'NEUTRAL'])]\n",
        "\n",
        "# Extract common n-grams (bi-grams)\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words='english', max_features=50)\n",
        "X = vectorizer.fit_transform(feedback_df['Review'])\n",
        "features = vectorizer.get_feature_names_out()\n",
        "counts = X.sum(axis=0).A1\n",
        "feature_freq = pd.DataFrame({'Feature': features, 'Frequency': counts}).sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "# Bar chart\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.barh(feature_freq['Feature'][:15][::-1], feature_freq['Frequency'][:15][::-1], color='salmon')\n",
        "plt.title(\"Top Feature Requests in NEGATIVE / NEUTRAL Reviews\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# WordCloud\n",
        "wc = WordCloud(width=1000, height=400, background_color='white').generate_from_frequencies(dict(zip(features, counts)))\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud of Feature Phrases\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "# Load enriched feedback (no Review_ID)\n",
        "df = pd.read_csv(\"hybrid_sentiment_output.csv\")\n",
        "# Ensure the correct sentiment column is used\n",
        "if 'Sentiment_Label' in df.columns:\n",
        "    label_col = 'Sentiment_Label'\n",
        "elif 'Sentiment_Lable' in df.columns:\n",
        "    label_col = 'Sentiment_Lable'\n",
        "else:\n",
        "    raise ValueError(\"❌ Neither 'Sentiment_Label' nor 'Sentiment_Lable' found in dataframe columns.\")\n",
        "\n",
        "# Focus on actionable feedback (NEGATIVE or NEUTRAL)\n",
        "feedback_df = df[df[label_col].isin(['NEGATIVE', 'NEUTRAL'])]\n",
        "\n",
        "# Extract common bi-grams and tri-grams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words='english', max_features=50)\n",
        "X = vectorizer.fit_transform(feedback_df['Review'].astype(str))  # Ensure it's string\n",
        "features = vectorizer.get_feature_names_out()\n",
        "counts = X.sum(axis=0).A1\n",
        "feature_freq = pd.DataFrame({'Feature': features, 'Frequency': counts}).sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "# Horizontal Bar Chart\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.barh(feature_freq['Feature'][:15][::-1], feature_freq['Frequency'][:15][::-1], color='salmon')\n",
        "plt.title(\"Top Feature Requests in NEGATIVE / NEUTRAL Reviews\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Word Cloud\n",
        "wc = WordCloud(width=1000, height=400, background_color='white').generate_from_frequencies(dict(zip(features, counts)))\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud of Feature Phrases\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "df = pd.read_csv(\"hybrid_sentiment_output.csv\")\n",
        "# Focus on actionable feedback (optional: only negative or neutral)\n",
        "feedback_df = df[df['Sentiment_Label'].isin(['NEGATIVE', 'NEUTRAL'])]\n",
        "\n",
        "# Extract common n-grams (bi-grams)\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words='english', max_features=50)\n",
        "X = vectorizer.fit_transform(feedback_df['Review'])\n",
        "features = vectorizer.get_feature_names_out()\n",
        "counts = X.sum(axis=0).A1\n",
        "feature_freq = pd.DataFrame({'Feature': features, 'Frequency': counts}).sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "# Bar chart\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.barh(feature_freq['Feature'][:15][::-1], feature_freq['Frequency'][:15][::-1], color='salmon')\n",
        "plt.title(\"Top Feature Requests in NEGATIVE / NEUTRAL Reviews\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# WordCloud\n",
        "wc = WordCloud(width=1000, height=400, background_color='white').generate_from_frequencies(dict(zip(features, counts)))\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud of Feature Phrases\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Step 1: Login with your Hugging Face token (replace with your token)\n",
        "login(\" \")  # <-- Replace this string with your real token\n",
        "\n",
        "# Step 2: Load your dataset\n",
        "df = pd.read_csv(\"hybrid_sentiment_output.csv\")\n",
        "\n",
        "# Step 3: Setup zero-shot pipeline, huggingface will use your logged-in token automatically\n",
        "# Check if GPU available, else CPU\n",
        "import torch\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "zero_shot = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    device=device,\n",
        "    use_auth_token=True  # explicitly use token to authenticate\n",
        ")\n",
        "\n",
        "topics = [\"user interface\", \"performance\", \"bugs\", \"features\", \"pricing\", \"ads\", \"login issues\"]\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def classify_batch(texts):\n",
        "    try:\n",
        "        results = zero_shot(texts, topics)\n",
        "        return [res['labels'][0] for res in results]\n",
        "    except Exception as e:\n",
        "        print(\"Batch classification error:\", e)\n",
        "        return [\"unclassified\"] * len(texts)\n",
        "\n",
        "reviews = df['Review'].fillna(\"\").astype(str).str[:512].tolist()\n",
        "\n",
        "all_topics = []\n",
        "print(\"🔍 Classifying review topics in batches...\")\n",
        "\n",
        "for i in tqdm(range(0, len(reviews), BATCH_SIZE)):\n",
        "    batch = reviews[i:i+BATCH_SIZE]\n",
        "    batch_topics = classify_batch(batch)\n",
        "    all_topics.extend(batch_topics)\n",
        "\n",
        "df['Topic'] = all_topics\n",
        "\n",
        "# Plot the results\n",
        "topic_counts = df['Topic'].value_counts().sort_values(ascending=True)\n",
        "plt.figure(figsize=(10, 6))\n",
        "topic_counts.plot(kind='barh', color='skyblue')\n",
        "plt.title(\"Most Discussed Topics in Reviews\")\n",
        "plt.xlabel(\"Number of Mentions\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "df.to_csv(\"llm_topic_classified_reviews.csv\", index=False)\n",
        "print(\"✅ Saved: llm_topic_classified_reviews.csv\")\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, y=\"Topic\", hue=\"Sentiment_Label\", order=df['Topic'].value_counts().index, palette=\"coolwarm\")\n",
        "plt.title(\"Sentiment Distribution by Topic\")\n",
        "plt.xlabel(\"Review Count\")\n",
        "plt.ylabel(\"Topic\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "top_negative = df[df[\"Sentiment_Label\"] == \"NEGATIVE\"][\"Topic\"].value_counts().head(5)\n",
        "print(\"Top 5 Problem Areas (Negative Sentiment):\")\n",
        "print(top_negative)\n",
        "\n",
        "\n",
        "summary = {\n",
        "    \"Total Reviews\": len(df),\n",
        "    \"Positive Reviews\": (df[\"Sentiment_Label\"] == \"POSITIVE\").sum(),\n",
        "    \"Negative Reviews\": (df[\"Sentiment_Label\"] == \"NEGATIVE\").sum(),\n",
        "    \"Neutral Reviews\": (df[\"Sentiment_Label\"] == \"NEUTRAL\").sum(),\n",
        "    \"Most Common Topic\": df[\"Topic\"].value_counts().idxmax()\n",
        "}\n",
        "\n",
        "for k, v in summary.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (if not already loaded)\n",
        "df = pd.read_csv(\"llm_topic_classified_reviews.csv\")\n",
        "\n",
        "# Filter negative reviews\n",
        "negative_df = df[df[\"Sentiment_Label\"] == \"NEGATIVE\"]\n",
        "\n",
        "# Load summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "# Define function to summarize text per topic\n",
        "def summarize_reviews(topic, max_reviews=20):\n",
        "    topic_reviews = negative_df[negative_df[\"Topic\"] == topic][\"Review\"].dropna().astype(str).tolist()\n",
        "    selected_reviews = topic_reviews[:max_reviews]  # avoid input length overflow\n",
        "    combined_text = \" \".join(selected_reviews)\n",
        "\n",
        "    if len(combined_text.strip()) < 30:\n",
        "        return \"Not enough data to summarize.\"\n",
        "\n",
        "    try:\n",
        "        summary = summarizer(combined_text[:1024], max_length=100, min_length=30, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing topic '{topic}':\", e)\n",
        "        return \"Summarization failed.\"\n",
        "\n",
        "# Summarize each topic\n",
        "topics = negative_df[\"Topic\"].unique()\n",
        "summaries = {}\n",
        "\n",
        "print(\"📄 Generating summaries for negative reviews per topic...\")\n",
        "\n",
        "for topic in topics:\n",
        "    summaries[topic] = summarize_reviews(topic)\n",
        "\n",
        "# Print the summaries\n",
        "for topic, summary in summaries.items():\n",
        "    print(f\"\\n🔹 Topic: {topic}\\nSummary: {summary}\\n\")\n",
        "\n",
        "\n",
        "with open(\"negative_review_summaries.txt\", \"w\") as f:\n",
        "    for topic, summary in summaries.items():\n",
        "        f.write(f\"Topic: {topic}\\nSummary: {summary}\\n\\n\")\n",
        "print(\"📁 Saved: negative_review_summaries.txt\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# --- Load your processed dataset ---\n",
        "df = pd.read_csv(\"hybrid_sentiment_output.csv\")\n",
        "\n",
        "# --- Clean & Normalize App_Name ---\n",
        "df[\"App_Name\"] = df[\"App_Name\"].astype(str).str.strip().str.lower()\n",
        "\n",
        "# Optional: Remove clearly invalid app names (symbols, blanks, etc.)\n",
        "df = df[df[\"App_Name\"].str.match(r\"^[a-zA-Z0-9\\s\\.\\-_:]+$\")]\n",
        "\n",
        "# --- Step 1: Sentiment distribution per app ---\n",
        "sentiment_summary = (\n",
        "    df.groupby(\"App_Name\")[\"Sentiment_Label\"]\n",
        "    .value_counts(normalize=True)\n",
        "    .unstack(fill_value=0)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Rename columns for clarity\n",
        "sentiment_summary.columns.name = None\n",
        "sentiment_summary.rename(columns={\n",
        "    \"POSITIVE\": \"Positive(%)\",\n",
        "    \"NEGATIVE\": \"Negative(%)\",\n",
        "    \"NEUTRAL\": \"Neutral(%)\"\n",
        "}, inplace=True)\n",
        "\n",
        "# --- Step 2: Total number of reviews per app ---\n",
        "review_counts = df[\"App_Name\"].value_counts().reset_index()\n",
        "review_counts.columns = [\"App_Name\", \"Total_Reviews\"]\n",
        "\n",
        "# --- Step 3: Most common topic per app (if 'Topic' exists) ---\n",
        "if \"Topic\" in df.columns:\n",
        "    top_topics = (\n",
        "        df.groupby(\"App_Name\")[\"Topic\"]\n",
        "        .agg(lambda x: x.value_counts().idxmax())\n",
        "        .reset_index()\n",
        "        .rename(columns={\"Topic\": \"Top_Topic\"})\n",
        "    )\n",
        "else:\n",
        "    top_topics = pd.DataFrame(columns=[\"App_Name\", \"Top_Topic\"])\n",
        "    print(\"⚠️ 'Topic' column not found. Skipping topic analysis.\")\n",
        "\n",
        "# --- Step 4: Merge summaries ---\n",
        "summary_df = sentiment_summary.merge(review_counts, on=\"App_Name\", how=\"left\")\n",
        "if not top_topics.empty:\n",
        "    summary_df = summary_df.merge(top_topics, on=\"App_Name\", how=\"left\")\n",
        "\n",
        "# --- Step 5: Round % columns and sort by Total Reviews ---\n",
        "for col in [\"Positive(%)\", \"Negative(%)\", \"Neutral(%)\"]:\n",
        "    if col in summary_df.columns:\n",
        "        summary_df[col] = (summary_df[col] * 100).round(1)\n",
        "\n",
        "summary_df = summary_df.sort_values(by=\"Total_Reviews\", ascending=False)\n",
        "\n",
        "# --- Step 6: Save output ---\n",
        "summary_df.to_csv(\"app_level_summary_report.csv\", index=False)\n",
        "print(\"✅ Saved: app_level_summary_report.csv\")\n",
        "\n",
        "# --- Optional Preview ---\n",
        "print(\"\\n📊 Top 5 Apps by Review Volume:\")\n",
        "print(summary_df.head())\n",
        "\n",
        "\n",
        "!pip install wordcloud\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data with full absolute path\n",
        "df = pd.read_csv(r\"D:\\project-LLM\\Data\\hybrid_sentiment_output.csv\")\n",
        "\n",
        "# Your existing dashboard code follows...\n",
        "\n",
        "\n",
        "df = pd.read_csv(r\"D:\\project-LLM\\Data\\hybrid_sentiment_output.csv\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset using the absolute path\n",
        "df = pd.read_csv(r\"D:\\project-LLM\\Data\\hybrid_sentiment_output.csv\")\n",
        "\n",
        "# Now you can add your Streamlit dashboard code below\n",
        "\n",
        "\n",
        "!pip install streamlit\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"hybrid_sentiment_output.csv\")\n",
        "\n",
        "# Handle missing 'Date' column\n",
        "if 'Date' not in df.columns:\n",
        "    st.warning(\"⚠️ 'Date' column not found. Generating random dates for demonstration.\")\n",
        "    df['Date'] = pd.to_datetime(np.random.choice(pd.date_range(\"2024-01-01\", \"2024-12-31\"), size=len(df)))\n",
        "else:\n",
        "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Sidebar filters\n",
        "st.sidebar.header(\"📊 Filter Options\")\n",
        "app_list = sorted(df[\"App_Name\"].dropna().unique())\n",
        "sentiments = sorted(df[\"Sentiment_Label\"].dropna().unique())\n",
        "topics = sorted(df[\"Topic\"].dropna().unique()) if \"Topic\" in df.columns else []\n",
        "\n",
        "# Filters\n",
        "selected_apps = st.sidebar.multiselect(\"Select App(s)\", app_list, default=app_list[:5])\n",
        "selected_sentiments = st.sidebar.multiselect(\"Select Sentiment(s)\", sentiments, default=sentiments)\n",
        "if topics:\n",
        "    selected_topics = st.sidebar.multiselect(\"Select Topic(s)\", topics, default=topics)\n",
        "search_term = st.sidebar.text_input(\"🔍 Search in Reviews\", \"\")\n",
        "\n",
        "# Filter dataset\n",
        "filtered_df = df[df[\"App_Name\"].isin(selected_apps) & df[\"Sentiment_Label\"].isin(selected_sentiments)]\n",
        "if topics:\n",
        "    filtered_df = filtered_df[filtered_df[\"Topic\"].isin(selected_topics)]\n",
        "if search_term:\n",
        "    filtered_df = filtered_df[filtered_df[\"Review\"].str.contains(search_term, case=False, na=False)]\n",
        "\n",
        "# Title\n",
        "st.title(\"📱 LLM Product Feedback Dashboard\")\n",
        "\n",
        "# View toggle\n",
        "view_option = st.radio(\"Select Visualization\", [\"Sentiment Breakdown\", \"Topic Distribution\", \"Word Cloud\", \"Sentiment Trend Over Time\"])\n",
        "\n",
        "# 1. Sentiment Breakdown\n",
        "if view_option == \"Sentiment Breakdown\":\n",
        "    st.subheader(\"📊 Sentiment Breakdown by App\")\n",
        "    top_apps = filtered_df[\"App_Name\"].value_counts().head(5).index.tolist()\n",
        "    subset = filtered_df[filtered_df[\"App_Name\"].isin(top_apps)]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.countplot(data=subset, x=\"App_Name\", hue=\"Sentiment_Label\", palette=\"Set2\")\n",
        "    plt.title(\"Sentiment Breakdown (Top 5 Apps)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "# 2. Topic Distribution\n",
        "elif view_option == \"Topic Distribution\":\n",
        "    st.subheader(\"🧠 Topic Distribution\")\n",
        "    if \"Topic\" in filtered_df.columns:\n",
        "        topic_counts = filtered_df[\"Topic\"].value_counts()\n",
        "        st.bar_chart(topic_counts)\n",
        "    else:\n",
        "        st.warning(\"⚠️ 'Topic' column not found in dataset.\")\n",
        "\n",
        "# 3. Word Cloud\n",
        "elif view_option == \"Word Cloud\":\n",
        "    st.subheader(\"☁️ Review Word Cloud\")\n",
        "    all_text = \" \".join(filtered_df[\"Review\"].dropna().astype(str))\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "# 4. Sentiment Trend Over Time\n",
        "elif view_option == \"Sentiment Trend Over Time\":\n",
        "    st.subheader(\"📈 Sentiment Trend Over Time\")\n",
        "    if 'Date' in filtered_df.columns:\n",
        "        trend = (\n",
        "            filtered_df.groupby([pd.Grouper(key='Date', freq='W'), 'Hybrid_Sentiment_Label'])\n",
        "            .size()\n",
        "            .unstack(fill_value=0)\n",
        "            .sort_index()\n",
        "        )\n",
        "        st.line_chart(trend)\n",
        "        st.caption(\"Track how user sentiment evolves weekly. Useful for monitoring app updates or campaigns.\")\n",
        "    else:\n",
        "        st.warning(\"⚠️ No 'Date' column available for time series analysis.\")\n",
        "\n",
        "# Show filtered data\n",
        "with st.expander(\"📄 View Filtered Data\"):\n",
        "    st.dataframe(filtered_df.reset_index(drop=True))\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"hybrid_sentiment_output.csv\")\n",
        "\n",
        "# Sidebar filters\n",
        "st.sidebar.header(\"📊 Filter Options\")\n",
        "\n",
        "# Unique values for filters\n",
        "app_list = sorted(df[\"App_Name\"].dropna().unique())\n",
        "sentiments = sorted(df[\"Sentiment_Label\"].dropna().unique())\n",
        "topics = sorted(df[\"Topic\"].dropna().unique()) if \"Topic\" in df.columns else []\n",
        "\n",
        "# App filter\n",
        "selected_apps = st.sidebar.multiselect(\"Select App(s)\", app_list, default=app_list[:5])\n",
        "# Sentiment filter\n",
        "selected_sentiments = st.sidebar.multiselect(\"Select Sentiment(s)\", sentiments, default=sentiments)\n",
        "# Topic filter (if exists)\n",
        "if topics:\n",
        "    selected_topics = st.sidebar.multiselect(\"Select Topic(s)\", topics, default=topics)\n",
        "\n",
        "# Keyword search\n",
        "search_term = st.sidebar.text_input(\"🔍 Search in Reviews\", \"\")\n",
        "\n",
        "# Filter data\n",
        "filtered_df = df[df[\"App_Name\"].isin(selected_apps) & df[\"Sentiment_Label\"].isin(selected_sentiments)]\n",
        "if topics:\n",
        "    filtered_df = filtered_df[filtered_df[\"Topic\"].isin(selected_topics)]\n",
        "if search_term:\n",
        "    filtered_df = filtered_df[filtered_df[\"Review\"].str.contains(search_term, case=False, na=False)]\n",
        "\n",
        "st.title(\"📱 LLM Product Feedback Dashboard\")\n",
        "\n",
        "# Toggle view\n",
        "view_option = st.radio(\"Select Visualization\", [\"Sentiment Breakdown\", \"Topic Distribution\", \"Word Cloud\"])\n",
        "\n",
        "# 1. Sentiment Breakdown\n",
        "if view_option == \"Sentiment Breakdown\":\n",
        "    st.subheader(\"📊 Sentiment Breakdown by App\")\n",
        "    top_apps = filtered_df[\"App_Name\"].value_counts().head(5).index.tolist()\n",
        "    subset = filtered_df[filtered_df[\"App_Name\"].isin(top_apps)]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.countplot(data=subset, x=\"App_Name\", hue=\"Sentiment_Label\", palette=\"coolwarm\")\n",
        "    plt.title(\"Sentiment Breakdown (Top 5 Apps)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "# 2. Topic Distribution\n",
        "elif view_option == \"Topic Distribution\":\n",
        "    st.subheader(\"🧠 Topic Distribution\")\n",
        "    if \"Topic\" in filtered_df.columns:\n",
        "        topic_counts = filtered_df[\"Topic\"].value_counts()\n",
        "        st.bar_chart(topic_counts)\n",
        "    else:\n",
        "        st.warning(\"⚠️ 'Topic' column not found in dataset.\")\n",
        "\n",
        "# 3. Word Cloud\n",
        "elif view_option == \"Word Cloud\":\n",
        "    st.subheader(\"☁️ Review Word Cloud\")\n",
        "    all_text = \" \".join(filtered_df[\"Review\"].dropna().astype(str))\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "# Show raw data\n",
        "with st.expander(\"🗂 View Filtered Data\"):\n",
        "    st.dataframe(filtered_df.reset_index(drop=True))\n",
        "\n",
        "\n",
        "# 5. Model Evaluation – Confidence Distribution\n",
        "if \"Sentiment_Confidence\" in filtered_df.columns:\n",
        "    st.subheader(\"🎯 Sentiment Confidence Distribution\")\n",
        "\n",
        "    # Basic stats\n",
        "    st.write(\"**Confidence Summary Stats**\")\n",
        "    st.write(filtered_df[\"Sentiment_Confidence\"].describe())\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(filtered_df[\"Sentiment_Confidence\"], bins=20, kde=True, color='green')\n",
        "    plt.title(\"Distribution of Sentiment Confidence Scores\")\n",
        "    plt.xlabel(\"Confidence Score\")\n",
        "    plt.ylabel(\"Number of Predictions\")\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "    # Box plot per sentiment (if applicable)\n",
        "    if \"Hybrid_Sentiment_Label\" in filtered_df.columns:\n",
        "        st.write(\"**Confidence per Sentiment**\")\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        sns.boxplot(data=filtered_df, x=\"Hybrid_Sentiment_Label\", y=\"Sentiment_Confidence\", palette=\"Set2\")\n",
        "        plt.title(\"Confidence Scores by Sentiment\")\n",
        "        st.pyplot(plt.gcf())\n",
        "\n",
        "else:\n",
        "    st.warning(\"⚠️ 'Sentiment_Confidence' column not found. Add model confidence scores to enable evaluation.\")\n",
        "\n",
        "\n",
        "!pip install -q streamlit transformers sentencepiece wordcloud seaborn pyngrok\n",
        "\n",
        "# Authenticate with Hugging Face (optional for private models)\n",
        "from huggingface_hub import login\n",
        "login(\"your_hf_token_here\")  # Optional: only if needed\n",
        "\n",
        "\n",
        "!pip install -q streamlit transformers sentencepiece wordcloud seaborn pyngrok\n",
        "\n",
        "# Authenticate with Hugging Face (optional for private models)\n",
        "from huggingface_hub import login\n",
        "login(\" \")  # Optional: only if needed, add token here\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # upload `hybrid_sentiment_output.csv`\n",
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "!streamlit run app_dashboard.py &\n",
        "\n",
        "# Wait a few seconds then open ngrok tunnel\n",
        "public_url = ngrok.connect(port=8501)\n",
        "print(\"🌐 Streamlit is live at:\", public_url)\n",
        "\n",
        "\n",
        "!pip install pyngrok\n",
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "!streamlit run app_dashboard.py &\n",
        "\n",
        "public_url = ngrok.connect(port=8501)\n",
        "print(\"🌐 Streamlit is live at:\", public_url)\n",
        "\n",
        "\n",
        "from pyngrok import conf\n",
        "conf.get_default().auth_token = \" \"# enter token\n",
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "!streamlit run app_dashboard.py &\n",
        "\n",
        "public_url = ngrok.connect(port=8501)\n",
        "print(\"🌐 Streamlit is live at:\", public_url)\n",
        "\n",
        "\n",
        "# ✅ Step 1: Install and configure ngrok\n",
        "!pip install pyngrok\n",
        "from pyngrok import conf, ngrok\n",
        "\n",
        "# ✅ Step 2: Set your real token here\n",
        "conf.get_default().auth_token = \" \"#enter token\n",
        "\n",
        "# ✅ Step 3: Run Streamlit app in the background\n",
        "!streamlit run app_dashboard.py &\n",
        "\n",
        "# ✅ Step 4: Create tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"🌐 Streamlit is live at:\", public_url)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the uploaded CSV\n",
        "df = pd.read_csv(\"hybrid_sentiment_output.csv\")\n",
        "\n",
        "# Define patterns commonly used in feature requests\n",
        "feature_keywords = [\n",
        "    r\"\\bi wish\\b\",\n",
        "    r\"\\bi would love\\b\",\n",
        "    r\"\\bplease add\\b\",\n",
        "    r\"\\bwould be great\\b\",\n",
        "    r\"\\bit would be nice\\b\",\n",
        "    r\"\\bplease include\\b\",\n",
        "    r\"\\bhope you can\\b\",\n",
        "    r\"\\bcan you add\\b\",\n",
        "    r\"\\bit needs\\b\",\n",
        "    r\"\\bmissing feature\\b\"\n",
        "]\n",
        "\n",
        "# Compile the regex\n",
        "pattern = re.compile(\"|\".join(feature_keywords), flags=re.IGNORECASE)\n",
        "\n",
        "# Add a column 'Feature_Request' based on pattern match in 'Review'\n",
        "df[\"Feature_Request\"] = df[\"Review\"].fillna(\"\").apply(lambda x: bool(pattern.search(x)))\n",
        "\n",
        "# Filter only feature request reviews\n",
        "feature_requests_df = df[df[\"Feature_Request\"] == True]\n",
        "\n",
        "# Display result\n",
        "print(\"🔍 Found\", len(feature_requests_df), \"feature request reviews.\\n\")\n",
        "print(feature_requests_df[[\"App_Name\", \"Review\", \"Sentiment_Label\"]].head(10))\n",
        "\n",
        "# Optional: Save to new CSV\n",
        "# feature_requests_df.to_csv(\"/mnt/data/feature_requests_output.csv\", index=False)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"hybrid_sentiment_output.csv\")\n",
        "\n",
        "# Broader phrases (no strict \"I wish\", etc.)\n",
        "feature_keywords = [\n",
        "    r\"wish\",\n",
        "    r\"would love\",\n",
        "    r\"please add\",\n",
        "    r\"should have\",\n",
        "    r\"missing\",\n",
        "    r\"need.*feature\",\n",
        "    r\"need.*option\",\n",
        "    r\"can you add\",\n",
        "    r\"must have\",\n",
        "    r\"important feature\",\n",
        "    r\"could use\",\n",
        "    r\"would be helpful\",\n",
        "    r\"add.*support\",\n",
        "    r\"add.*option\",\n",
        "    r\"include.*feature\",\n",
        "    r\"hope.*add\",\n",
        "    r\"should include\",\n",
        "    r\"more.*features\"\n",
        "]\n",
        "\n",
        "pattern = re.compile(\"|\".join(feature_keywords), flags=re.IGNORECASE)\n",
        "\n",
        "# Apply pattern search\n",
        "df[\"Feature_Request\"] = df[\"Review\"].fillna(\"\").apply(lambda x: bool(pattern.search(x)))\n",
        "\n",
        "# Filter\n",
        "feature_requests_df = df[df[\"Feature_Request\"] == True]\n",
        "\n",
        "# Show results\n",
        "print(f\"🔍 Found {len(feature_requests_df)} feature request reviews.\\n\")\n",
        "print(feature_requests_df[[\"App_Name\", \"Review\", \"Sentiment_Label\"]].head(10))\n",
        "\n",
        "\n"
      ]
    }
  ]
}